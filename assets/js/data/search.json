[ { "title": "Mastering Asynchronous Programming with Kotlin Coroutines — Part 1", "url": "/posts/ff56d35a891f/", "categories": "", "tags": "kotlin, kotlin-coroutines, threads", "date": "2024-03-25 21:13:00 +0000", "snippet": "Mastering Asynchronous Programming with Kotlin Coroutines — Part 1 Welcome to mastering Asynchronous Programming with Kotlin Coroutines — Part 1. In this article, we’re going to cover coroutine ba...", "content": "Mastering Asynchronous Programming with Kotlin Coroutines — Part 1 Welcome to mastering Asynchronous Programming with Kotlin Coroutines — Part 1. In this article, we’re going to cover coroutine basics, coroutine usage, and coroutine builders launch , async , and runBlocking .Introduction to CoroutinesWhen an app starts, it initiates a main thread responsible for handling lightweight tasks like button clicks or user login. However, if the app needs to execute a lengthy operation such as downloading a file, or network calls, doing so on the main thread can cause the app to become unresponsive, leading to a poor user experience. To counter this situation, we should run background threads to handle these tasks. However, as each thread consumes a significant amount of memory, running a lot of background threads can lead to out-of-memory errors.Here comes coroutines to the rescue. Coroutines act as lightweight threads, offering a more efficient solution compared to traditional threads. They are designed to be cheap and consume minimal memory. One of the key advantages of coroutines is their ability to be launched on a single thread and perform multiple operations without blocking other coroutines.Multiple threads performing one operation at a timeIn the above diagram, multiple threads are launched to perform various operations. Below is the code to create threads in Kotlin using thread keywords.fun main() { println(\"Main thread starts here : ${Thread.currentThread().name}\") // Launching a new thread to offload work from the main thread thread { println(\"Fake work starts here : ${Thread.currentThread().name}\") Thread.sleep(1000) // some fake work like file downloading or n/w call etc. println(\"Fake work finished here : ${Thread.currentThread().name}\") } println(\"Main thread ends here : ${Thread.currentThread().name}\")}Main thread starts here : mainMain thread ends here : mainFake work starts here : Thread-0Fake work finished here : Thread-0One thing to note here is that although the main thread has finished its work it will still wait for other threads to finish the work.Multiple coroutines performing operations on a single threadfun main() { println(\"Main thread starts here : ${Thread.currentThread().name}\") // Launching a new coroutine to offload work from the main thread createCoroutine() println(\"Main thread ends here : ${Thread.currentThread().name}\")}fun createCoroutine() { // operates with in a thread GlobalScope.launch { println(\"Fake coroutine starts here : ${Thread.currentThread().name}\") Thread.sleep(1000) // some fake work println(\"Fake coroutine finished here : ${Thread.currentThread().name}\") }}Main thread starts here : mainMain thread ends here : mainSo, it’s evident that coroutines enable asynchronous execution without blocking the main thread. However, in this scenario, we didn’t achieve the desired outcome from the coroutine. Although we launched a coroutine using the createCoroutine function, it didn’t print anything. We need the main thread to wait until the execution of all the coroutines is completed. One simple solution is to deliberately add a delay in the main thread using thread.Sleep() to ensure that the coroutine finishes its work. But, this is an impractical solution as we can’t always predict the time required for the coroutine to finish its task. Blocking the main thread with a fixed delay is also not efficient and may lead to unnecessary waiting or potential responsiveness issues in the application.fun main() { println(\"Main thread starts here : ${Thread.currentThread().name}\") // Launching a new coroutine to offload work from the main thread createCoroutine() Thread.sleep(2000) println(\"Main thread ends here : ${Thread.currentThread().name}\")}fun createCoroutine() { // operates with in a thread GlobalScope.launch { println(\"Fake coroutine starts here : ${Thread.currentThread().name}\") Thread.sleep(1000) // some fake work println(\"Fake coroutine finished here : ${Thread.currentThread().name}\") }}Main thread starts here : mainFake coroutine starts here : DefaultDispatcher-worker-1Fake coroutine finished here : DefaultDispatcher-worker-1Main thread ends here : main2. We can use thread.join call to wait for all coroutines to finish the work before the main thread terminates.fun main() = runBlocking { // this: CoroutineScope println(\"Main thread starts here : ${Thread.currentThread().name}\") val job: Job = GlobalScope.launch {// it can launch or GlobalScope.launch based on the requirement println(\"Fake coroutine starts here : ${Thread.currentThread().name}\") doWork(1000) println(\"Fake coroutine finished here : ${Thread.currentThread().name}\") } Thread.sleep(2000) job.join() // wait for the coroutine to finish // job.cancel() // cancel the coroutine println(\"Main thread ends here : ${Thread.currentThread().name}\") }suspend fun doWork(time : Long) { delay(time) // some fake work}In the above code snippet, we’ve used job.join() instead of the sleep() function.Let’s move to the next topic now how to create coroutines in our application.How to create CoroutinesCoroutines in Kotlin are created using Coroutine builders. These are functions or constructs provided by Kotlin’s coroutine library that allow the creation and management of coroutines. These builders simplify the process of launching and managing coroutines, providing different options based on specific use cases.Some common coroutine builders include:Common Coroutine BuildersBefore going into how to use these builders to create coroutines, let’s understand the concept of scope of these builders first. In Kotlin, when dealing with coroutines, scope refers to the context in which a coroutine runs and is controlled. There are primarily two types of scopes relevant to coroutines: GlobalScope and CoroutineScope. GlobalScope: It is a top-level scope that is not tied to any specific lifecycle or context. Coroutines launched in global scope continue to execute until they are complete or until the application terminates. It’s generally recommended to avoid using GlobalScope in production code because coroutines launched in GlobalScope can potentially run indefinitely and may lead to resource/memory leaks or unintended behavior. CoroutineScope: This is a scope tied to a specific coroutine builder, such as launch or async . When the associated object is destroyed or when the scope is canceled, all coroutines launched within that scope are automatically canceled. When the thread is closed, all the coroutines associated with that thread are also closed/destroyed.Launch BuilderThe launch builder launches a coroutine having a return type job . This job object can be used to perform various other operations like join and cancel (which will be discussed in the next part) .Here, we use GlobalScope.launch to create a coroutine with a global scope. Alternatively, we can use launch to create a coroutine with a coroutine scope.fun main() = runBlocking { // this: CoroutineScope println(\"Main thread starts here : ${Thread.currentThread().name}\") val job: Job = GlobalScope.launch {// it can launch or GlobalScope.launch based on the requirement println(\"Fake coroutine starts here : ${Thread.currentThread().name}\") doWork(1000) println(\"Fake coroutine finished here : ${Thread.currentThread().name}\") } Thread.sleep(2000) job.join() // wait for the coroutine to finish // job.cancel() // cancel the coroutine println(\"Main thread ends here : ${Thread.currentThread().name}\") }suspend fun doWork(time : Long) { delay(time) // some fake work}Main thread starts here : mainFake coroutine starts here : DefaultDispatcher-worker-1Fake coroutine finished here : DefaultDispatcher-worker-1Main thread ends here : mainHere, we’ve used the delay() function instead of Thread.sleep() in the doWork() function. The difference between the delay() and sleep() functions will be discussed later in this article.Pros Lightweight and efficient for fire-and-forget tasks. No overhead in managing a result.Cons : Lack of result handling may complicate error management. Careful usage is required to avoid resource leaks.Async BuilderThe async builder creates a coroutine that computes a result asynchronously and returns a deferred result just like the future in other programming languages. You need to use await() function to retrieve the corresponding result.fun main() = runBlocking { // this: CoroutineScope println(\"Main thread starts here : ${Thread.currentThread().name}\") val deferred: Deferred&lt;String&gt; = async { println(\"Fake coroutine (Join) starts here : ${Thread.currentThread().name}\") doWork(1000) println(\"Fake coroutine (Join) finished here : ${Thread.currentThread().name}\") \"deffered job\" } val jobType: String = deferred.await() // wait for the coroutine to finish and returns the result deferred.join() // wait for the coroutine to finish println(\"Job type is $jobType\") println(\"Main thread ends here : ${Thread.currentThread().name}\")}suspend fun doWork(time : Long) { delay(time) // some fake work}Main thread starts here : mainFake coroutine (Join) starts here : mainFake coroutine (Join) finished here : mainJob type is deffered jobMain thread ends here : mainIn this example, we retrieve the result using deferred.await() call, where the return type can be of any data type. Instead of thread.sleep , we have used join function call to wait for all coroutines to finish the work before the main thread terminates.Pros : Facilitates multiple concurrent computations with easy result retrieval.RunBlocking BuilderThe runBlocking coroutine builder creates a new coroutine and blocks the current thread until its execution is complete. It is typically used in testing, main functions, or blocking code that needs to be integrated into coroutine-based systems. It is mainly used to test the suspending functions (details of run blocking will be covered in the next part of the blog alongside suspending functions) . For now just remember — It can only be called by coroutines and suspend functions.class RunBlockingTest { @Test fun `test fetchData`() = runBlocking { val result = fetchData() Assert.assertEquals(\"Mock data\", result) }}suspend fun fetchData(): String { // Simulate fetching data asynchronously return \"Mock data\"}In this example, the fetchData function is a suspending function and a suspend function can only be called by the coroutines or suspend functions. We are using runBlocking here to create a coroutine to test fetchData function.Difference between Sleep and Delay functionThe delay function is an alternative to the sleep function because thread.sleep() makes the entire thread sleep rather than blocking the corresponding coroutine only. The delay function is a type of suspend function, that allows us to pause the execution of a coroutine for a specified amount of time without blocking the underlying thread.Difference between sleep and delay functionIn the above diagram, coroutine c1 has called thread.sleep function but it has suspended the main function and all the coroutines associated with it. But delay function has suspended the execution of coroutine c1 only. In conclusion, Kotlin coroutines offer an efficient solution for asynchronous programming, providing lightweight threads and simplifying concurrency. Coroutine builders like launch , async , and runBlocking facilitate various use cases, but care must be taken to avoid blocking issues and resource leaks. Asynchronous code can be tested synchronously using runBlocking , enhancing simplicity and ease of testing in coroutine-based systems.Thank you for taking the time to read. I hope you found it insightful and engaging. Keep an eye out for the next parts!Post converted from Medium by ZMediumToMarkdown." }, { "title": "Demystifying Kubernetes: Part 1", "url": "/posts/40902c487f04/", "categories": "", "tags": "kubernetes, kubernetes-cluster, kubectl, kubernetes-security", "date": "2023-07-22 11:13:36 +0000", "snippet": "Demystifying Kubernetes: Part 1In the last blog , we discussed docker using which you can package your application along with all its dependencies as images and you can directly run those images in...", "content": "Demystifying Kubernetes: Part 1In the last blog , we discussed docker using which you can package your application along with all its dependencies as images and you can directly run those images in containers.But running containers is not everything, you need some kind of a tool to up/downscale or monitor them. So we need a tool to harness the power of containers, That’s where Kubernetes do the magic.Kubernetes was first developed by a team at Google. Later on, it was donated to Cloud Native Computing Foundation (CNCF) . Kubernetes is an orchestrator tool for containerized applications. The next question is what is an orchestrator?An Orchestrator, just another fancy word, is a system that takes care of the deployment and management of multiple applications in a distributed environment.Docker is the low-level technology that provides container runtime while k8s is the high-level technology that takes care of all other stuff like monitoring, self-healing, scale-up-down, etc.Benefits of k8s : It makes sure that resources are used efficiently and within the constraints defined by the user. Scale up and down resources based on the demand. It provides high availability and zero-downtime rolling code changes. Self-healing, service discovery.and a lot more…….Before delving into practical examples, Let’s discuss k8s in detail first.Major K8s components :You package your application as a container and deploy it on the Kubernetes cluster. It is a common practice for managing scalable, containerized applications. Kubernetes uses a master-worker architecture, where the control plane nodes handle cluster management and coordination, while the worker nodes run the application containers. Don’t worry if these names sound fancy to you, you will learn about them as we progress through the blog.Let’s dig deeper into each of the components:Control Plane:Imagine the Control Plane in Kubernetes as the “brain” of the whole operation. It’s like the central command center that manages and coordinates everything that happens in the Kubernetes cluster. Desired State Management: The Control Plane stores and manages the desired state of the cluster based on user-provided YAML files. Users define the desired state using YAML files, specifying configurations for applications and resources. Declarative Approach : Kubernetes follows a declarative approach, where users declare what they want, and Kubernetes handles the implementation details. Continuous Monitoring : The Control Plane continuously monitors the cluster’s actual state. It compares desired state(defined in the YAML file) with the current state and takes corrective actions for any discrepancies. Self-Healing and Resilience : The Control Plane automatically manages the cluster, responding to changes and recovering from failures.apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-container image: nginx:latest ports: - containerPort: 80In this example, the YAML file defines a Kubernetes Deployment with three replicas of the Nginx web server. The Control Plane ensures the desired state is maintained by creating and managing the specified pods. Don’t worry about the terms defined in the YAML file as of now.Let’s take a quick look at the different components making up the control plane. API Server The API Server is like the main communication hub in the cluster. It exposes the Kubernetes API , which is like a set of rules and instructions for interacting with the cluster. You can use commands or tools to talk to the API Server and ask it to create, update, or delete resources like pods, services, and deployments. The API server is a frontend of the control plane which exposes a set of RESTful endpoints, handling all the communication with the cluster.2. etcd : etcd is a popular distributed database, that acts as a memory of the cluster and is a single source of truth for a cluster. It keeps track of the configuration data and the current state of the cluster. It’s highly reliable and ensures that even if a part of the cluster fails, the information is still safe and available . etcd is a distributed “brain” storing cluster configuration data and current state.3. Controller Manager :A control loop is a mechanism used by the Controller Manager. The control loop monitors the cluster’s current state and compares it with the actual state and corrects any discrepancies between the desired and actual states of resources. It is just like a thermostat in your home that continuously monitors the temperature and adjusts the heating or cooling to keep it close to the desired setting. The controller manager works similarly to ensure the cluster stays in the desired state. The Controller Manager is like a watchful supervisor for the cluster. It constantly looks at the cluster’s current state and compares it to the desired state you’ve specified. If there’s any difference between the two states, the Controller Manager takes action to bring everything back in line with what you want. Controller Manager is a watchful manager, always checking if the cluster is as you desire and making adjustments when needed.4. Scheduler : Imagine the Scheduler as a smart “matchmaker” for your app containers and worker nodes. When you want to run a new container or scale your app, the Scheduler finds the best place (worker node) to put it using some complex logic behind the scenes. It looks at the available resources on each node, like memory and CPU , and ensures the containers are placed efficiently, making the best use of the cluster’s capacity. Scheduler is like a wise organizer, it assigns the right tasks (pods) to the best workers (nodes) in the cluster.Control Plane and worker NodesWorker NodesIn a Kubernetes cluster, worker nodes are like the hands-on workers that do the actual job of running your applications. They are the ones responsible for executing the containers that make up your software. Each worker node works together as part of the cluster, with the control plane overseeing and guiding their tasks.The Role of Worker Nodes: Running Applications : Worker nodes handle the real work by running your applications and services in containers. Cluster Members : They are active members of the Kubernetes cluster, collaborating with the control plane to keep everything organized. Physical or Virtual Machines : Worker nodes can be either physical machines or virtual machines (VMs) depending on how your cluster is set up. Scalability and Load Sharing : More worker nodes mean the cluster can handle bigger tasks and share the workload among different machines. Redundancy for Resilience : Having multiple worker nodes adds redundancy, making sure that if one worker node has a problem, the others can keep the applications running smoothly.In short, worker nodes are the heart of the Kubernetes cluster, working hard to run your applications while following the guidance of the control plane. They provide the necessary processing power and resources needed to keep your software running efficiently and reliably.Let’s take a quick look at the different components making up the worker nodes:Components of Worker Nodes in Kubernetes: Container Runtime (e.g., Docker, containerd): Efficient Execution : The container runtime provides an efficient way to execute your applications. It uses container technology, which allows your apps to run in isolated environments, ensuring that they don’t interfere with each other. Easy Management : Managing applications becomes easier with container runtime. You can start, stop, or update containers quickly without affecting other parts of your cluster. Portability and Consistency: Containers created by the runtime are consistent across different environments. This portability allows you to develop locally and then seamlessly move your applications to the cloud or other Kubernetes clusters.2. Kubelet : Obedient Worker : The kubelet is like a diligent worker on each node, taking instructions from the control plane. It ensures that the containers specified in the YAML files are running as desired. Health Monitor: The kubelet continuously monitors the health of containers. If a container crashes or becomes unresponsive, the kubelet automatically restarts it to maintain the desired state. Resource Allocator : With its resource management capabilities, the kubelet allocates the right amount of CPU, memory, and other resources to each container. This ensures that containers run efficiently without overloading the node.3. Kube-proxy: Networking Magician : Kube-proxy is like a networking magician, managing the necessary network rules to enable communication between your containers and services within the Kubernetes cluster. It sets up network routes and connections so that containers can find and talk to each other. Load Balancer : When you have multiple replicas of an application, kube-proxy acts as a load balancer, distributing incoming traffic among those replicas. This load balancing ensures that your application can handle more users and traffic while maintaining performance and availability. High Availability : Kube-proxy ensures high availability for your applications. If one container or node fails, kube-proxy automatically redirects the traffic to healthy instances, providing fault tolerance and preventing disruptions.Interaction between Control Plane and Worker Nodes:The control plane and worker nodes in Kubernetes work together like a well-coordinated team to make sure everything runs smoothly. The control plane, which acts as the cluster’s “brain,” keeps track of how things should be according to the plans you provide in the YAML files. It keeps an eye on what’s happening in the cluster all the time. The worker nodes, like hardworking helpers, follow the control plane’s instructions. They run your applications and tell the control plane how things are going. The control plane then guides the worker nodes, telling them what to do, like creating or removing containers. The worker nodes also tell the control plane about available resources and how healthy things are. This back-and-forth communication helps the cluster stay organized and ensures that your apps run just the way you want them to.SummaryIn summary, Kubernetes is a powerful system that manages applications in containers. We’ve covered the main parts, like the control plane and worker nodes, that work together to make sure everything runs smoothly. But there’s a lot more to explore! In the next part of this series, we’ll dive deeper into Kubernetes with practical examples. We’ll learn how to deploy applications, manage resources, and handle networking. By the end, you’ll have a better understanding of Kubernetes and how to use it effectively for your applications. Stay tuned for the next part, where we’ll take this journey together!Post converted from Medium by ZMediumToMarkdown." }, { "title": "Database Isolation levels", "url": "/posts/c27eb9b33217/", "categories": "", "tags": "database, isolation-level, locking, mysql, readwritelock", "date": "2023-07-18 17:15:16 +0000", "snippet": "Database Isolation levelsImage source: GoogleManaging queries on a database becomes challenging when multiple users are trying to fetch/update information concurrently. As your application grows an...", "content": "Database Isolation levelsImage source: GoogleManaging queries on a database becomes challenging when multiple users are trying to fetch/update information concurrently. As your application grows and attracts more users, there is a need to ensure integrity and prevent conflicts. Here database locking comes into the picture to address these concerns. It is one of the fundamental aspect of the ACID properties of a database.Isolation levels define the degree to which transactions are isolated from one another. Isolation is achieved by using locks to control concurrent access to data. Locks are mechanisms used to control access to data and prevent simultaneous modifications that could result in inconsistencies or errors.Why do we need database isolation?Let’s say we want to implement e-commerce website handling, where numerous operations occur simultaneously. Database isolation plays a crucial role in maintaining consistency. Here’s how isolation controls various aspects to ensure data integrity: Isolation levels determine the duration of read locks , balancing concurrency and consistency. Example: Holding a read lock for a short duration to allow multiple customers to view product information simultaneously. Isolation controls whether locks are taken when reading data and the type of locks requested, preventing conflicts. Example: Acquiring a lock when a customer wants to purchase a limited-stock item to avoid overselling. Isolation determines whether a read operation referencing modified data blocks until the exclusive lock is released or retrieves the committed version. Example: Blocking a read operation until a transaction updating the product price completes to ensure accurate pricing information. High isolation levels, like serializable, provide strong consistency by isolating transactions completely. Example: Ensuring that concurrent customers purchasing the same item see consistent inventory availability. Lower isolation levels, such as read committed or read uncommitted, prioritize concurrency but may allow reading uncommitted or inconsistent data . Example: Allowing customers to view product details even if another customer is in the process of updating them. Selecting the appropriate isolation level balances data integrity and concurrency based on specific requirements. Example: Choosing a higher isolation level for critical operations like processing payments and a lower level for less critical actions like viewing product descriptions. Database isolation ensures that concurrent operations, such as purchasing, price changes, and product deliveries, are handled consistently and accurately in a large e-commerce system.Problems without proper isolationWithout proper isolation, conflicts and data inconsistencies can occur. Let’s explore these scenarios, we will be using an account table to illustrate scenarios.account table Dirty Read : A dirty read occurs when one transaction reads data that has been modified by another transaction but has not been committed yet.Dirty ReadHere T2 reads the value of a , while T1 is still updating it. If T2 uses this uncommitted data for further processing, it may lead to incorrect calculations or decisions. To demonstrate dirty reads, we need to set the isolation level to READ UNCOMMITTED.Dirty ReadHere I have not committed T1(updating the value of balance to 100), still, T2 is able to read uncommitted data.2. Non-Repeatable Reads: A transaction reads the same data multiple times within its duration and obtains different results each time. This inconsistency arises due to other concurrent transactions modifying and committing the data between the reads.Non-Repeatable ReadHere T1 reads the value of a , while T2 is still updating it and after T2 commits its changes ,T1 reads the updated value of a in the same transaction. To demonstrate non-repeatable reads, you can use the “Read Committed” isolation level. However, it prevents dirty reads.Non-repeatable readsThe “Read Committed” isolation level allows each read to see only the committed data at the time of reading. If data is modified or deleted and committed between the reads within the same transaction, non-repeatable reads can occur.3. Phantom Reads: Phantom reads happen when a transaction retrieves a set of rows based on a certain condition, but another concurrent transaction inserts or deletes rows that meet the same condition. As a result, the set of rows appears to change “phantomly.”Phantom ReadHere T1 fetches the rows of table A , while T2 is still updating it and after T2 commits its changes ,T1 fetches the rows of table A in same transaction and number of rows are different for same query.Phantom ReadsIn above example, output1 != output2 for the same query in same transaction.Database isolation levelTo prevent all above scenarios, we need to define degree of isolation at database level. They determine how transactions interact with each other and the level of data consistency and integrity guaranteed by the system. Here are the commonly used database isolation levels: Read Uncommitted: We have already discussed it while discussing Dirty read . Please refer dirty read section to know more. This is the lowest level of isolation, and does almost nothing. Allows transactions to read uncommitted and potentially inconsistent data modified by other concurrent transactions. Does not enforce any locks, allowing for high concurrency but risking dirty reads, non-repeatable reads, and phantom reads.Read Uncommited2. Read Committed : We have already discussed it while discussing Non-Repeatable Reads and Phantom read. Please refer that section to know more. Ensures that transactions only read committed data, preventing dirty reads. Acquires read locks on accessed data, preventing dirty reads but allowing non-repeatable reads and phantom reads.Read committed3. Repeatable Read : The most popular isolation level is REPEATABLE_READ , Makes sure that any transaction that reads data from a row, blocks any other concurrent writing transactions from accessing the same row. Acquires locks on accessed data, preventing dirty reads and non-repeatable reads but allowing phantom reads.Repeatable ReadRepeatable readHere output1 and output2 for the same query in same transaction fetches different number of rows.4. Serializable : Provides the highest level of isolation by ensuring that transactions execute as if they were serialized, one after the other. Acquires range locks or table-level locks, preventing dirty reads, non-repeatable reads, and phantom reads , but may result in decreased concurrency.SerializableChoice of transaction isolation levelHere are the key points to consider when choosing a transaction isolation level: Data Consistency : Higher isolation levels like REPEATABLE READ or SERIALIZABLE provide stronger consistency guarantees by preventing concurrency anomalies. Concurrency : Lower isolation levels like READ COMMITTED or READ UNCOMMITTED allow for higher concurrency but may introduce potential anomalies like dirty reads, non-repeatable reads, or phantom reads. Application Requirements : Consider the specific data integrity needs and critical operations of your application to determine the appropriate isolation level. Performance : Higher isolation levels may impact performance due to increased locks and reduced concurrency. Evaluate the performance implications for your specific use case.Post converted from Medium by ZMediumToMarkdown." }, { "title": "Learn Docker through Hands-on Examples", "url": "/posts/496955847700/", "categories": "", "tags": "docker, docker-compose, dockerfiles, docker-image", "date": "2023-07-11 20:56:44 +0000", "snippet": "Learn Docker through Hands-on ExamplesImage Source: Google Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized...", "content": "Learn Docker through Hands-on ExamplesImage Source: Google Docker is a software platform that allows you to build, test, and deploy applications quickly. Docker packages software into standardized units called containers that have everything the software needs to run including libraries, system tools, code, and runtime.ContainersNow the next question is what is a container and what problems does it solve? A container is a method to package applications with all the required dependencies and configurations. It is a portable artifact that can be easily shared and moved between different environments. Containers enhance development and deployment efficiency.Where do containers live?As we know Containers, being portable, can be easily shared and moved around. They can be stored in container repositories, such as Docker Hub, Amazon ECR, and Google Container Registry. Private repositories cater to company-specific needs, ensuring controlled access and security. Public repositories like Docker Hub offer a wide range of pre-built application containers, enabling easy discovery and utilization.Now, let’s see how containers improved the deployment process:Before the introduction of containers: Applications were individually installed on local systems, leading to inconsistencies and potential conflicts. Dependency management was challenging, with compatibility issues and conflicts between different versions of libraries. Portability was limited, requiring manual adjustments when moving applications across different environments, leading to the “works on my machine” problem.After the introduction of containers: Dependency installation is no longer required on the host system as containers have their own isolated OS layer and include all necessary dependencies and configurations. The same command can be used to fetch and install the application within the container, regardless of the underlying operating system. This eliminates the need for manual configuration and ensures consistent behavior across different environments, making development and deployment more streamlined and efficient.Application DeploymentNow let’s see how containers ease the deployment processDeployment Process Before the Introduction of Containers: Manual artifact creation and deployment instructions. Manual setup of the deployment environment, including infrastructure components. Potential dependency version conflicts on the operating system. Reliance on textual instructions leads to manual errors or misunderstandings. Time-consuming and error-prone environment configuration. Lack of standardized deployment units, resulting in inconsistencies and compatibility issues.Deployment Process After the Introduction of Containers: Containerized artifacts ensure application portability and consistency. Infrastructure as code enables automated provisioning and configuration of deployment environments. Automated deployment with orchestration tools like Kubernetes improves scalability and reliability. Integration with CI/CD pipelines allows for seamless and efficient application testing, building, and deployment. So technically, Container is layer of stacked images on top of each other.Let’s delve into practical examples.Let’s say, I want to install Postgres, I will install it using the docker image.As you can see from the below image, it is unable it find it locally and will pull it from the docker hub. The output shows different layers of images, represented by hashes, being downloaded. These layers can include the underlying Linux image as well as the application-specific layers.The advantage of using different layers of images is : Incremental Updates : Only updated layers are downloaded, saving time and bandwidth when installing different Postgres versions. Efficient Storage Utilization : Common base layers are shared among images, minimizing redundancy and optimizing storage usage. Faster Deployment and Versioning : Docker identifies unchanged layers for quicker deployment and supports versioning with preserved layers.Downloading Postgres imageSee the output of the below docker ps command:portThere are two terms here, container and image. Image : An image is a standalone, read-only template that contains the application code, dependencies, and configurations. It serves as the basis for creating containers. Container : A container is a running instance of an image. It is an isolated and executable environment that includes the necessary runtime components and allows the application to run and interact with the surrounding system. In summary, an image is a static template, while a container is a dynamic and running instance created from an image.Docker vs Virtual MachineOS has 2 layers, What part of OS both virtualize?Docker : Virtualizes the application layer of the operating system (OS) . When a Docker image is downloaded, it includes the application layer of the OS and other applications installed on top of it. Utilizes the host system’s kernel, rather than running its own separate kernel.Virtual Machines (VMs): Virtualizes both the application layer and the kernel of the OS. Creates a complete virtualized instance of an OS, running its own separate kernel. Does not rely on the host system’s kernel and operates as an independent virtualized environment. Benefits of Docker over VMs is that Docker containers offer faster startup times and efficient memory utilization compared to traditional virtual machines.Basic Docker Commands: docker ps : Lists all running containers. docker run [image] : Creates and starts a new container based on the specified image. docker stop [container] : Stops a running container. docker start [container] : Starts specified container. docker ps -a: lists all containers, including both running and stopped containersPort BindingLet’s say I need two different versions of Postgres for different applications and I started two containers of Postgres as shown below:two containers bound to the same portIn the scenario where you require two different versions of Postgres for different applications and have started two Postgres containers, both running on the same port, the challenge arises as to how the two different applications will communicate with their respective Postgres containers. This is where port binding becomes important.Let’s see how it works.There are container ports and host ports. When running multiple containers on your host machine, each container can expose its own set of ports. To establish communication between the containers and the host machine, a binding or mapping needs to be created between the port on the host machine and the port exposed by the container. This allows the applications running inside the containers to interact with the specified ports on the host machine.You can create those bindings as shown below:port bindingNow two different versions of Postgres are associated with the 6001 and 6000 ports of the host machine.Docker LogsThe docker logs command is helpful for troubleshooting, debugging, and monitoring containerized applications, providing insights into their runtime behavior and any issues that may have occurred.Syntax: docker logs [container]Docker ExecThe docker exec command allows you to run a command within a running container and get access to its terminal.It is useful for running commands or interacting with a running container, allowing you to execute commands as if you were directly accessing the container’s terminal.Docker exec commandPractical ExampleWe will be coding a Go application using MongoDB for a Docker demonstration. It will expose APIs to insert/fetch data from MongoDB.Golang ApplicationWe need to run the MongoDB container for the application to access and also to connect it to mongo express. We need both of them to be able to communicate with each other. In order to facilitate communication between the MongoDB container and the Mongo Express container, it’s essential to understand the concept of Docker Networking.Docker creates its own isolated network for containers to run in. Containers within the same network (mongodb and mongoexpress in our case), can communicate with each other using their container names without relying on host-specific details like IP addresses or ports.The Go application, running outside the Docker network, can connect to the MongoDB and Mongo Express containers using their respective hostnames and ports. It communicates with them as if they were external services accessible through the network.Docker NetworkBelow are the commands to create a docker network and bind MongoDB and mongo-express to itdocker network and bind Mongo to itbind mongo-express to the same networkCreate your own database to mongo express hosted on localhost:8081 as shown below :MongoDB expressDocker ComposeWe executed the below commands to create a docker network and start containers.Commands to execute Mongo and ExpressThis way of starting docker containers all the time is a little bit tedious and error-prone. You don’t want to run all these commands manually every time you want to run your application. Here docker-compose comes to the rescue.Below is the docker-compose YAML file. Docker Compose can greatly simplify the process of starting and managing Docker containers. It allows you to define and run multi-container applications with a single YAML file.docker-composeRun the docker-compose file.Run using docker-composeHere the interesting thing to note is that it is creating a network ‘Network docker-tutorial-mongodb_default’ to run both the containers in it. we can verify it using docker ps.Build our own DockerfileNow our application is ready to deploy, in order to do so, our application should be packaged into its own docker image.In order to build a Docker image, we need Dockerfile .Dockerfiledocker build -t [image:tag] [location of docker file]docker build -t myapp .docker run myapp // run the applicationIt will create a docker image.Docker RegistryA Docker Registry is a repository that stores Docker images, allowing users to share, distribute, and manage container images across different environments and systemsWe will be pushing the above-generated image to the AWS container registery. Set up an AWS account and create an ECR repository. (As shown below) Authenticate the Docker CLI with your AWS credentials by running the aws ecr get-login-password --region &amp;lt;region&amp;gt; | docker login --username AWS --password-stdin &amp;lt;account-id&amp;gt;.dkr.ecr.&amp;lt;region&amp;gt;.amazonaws.com command.3. Build the docker image using the commands shown below:Push commandsOnce the above commands are executed, you can see the docker image of your app on the AWS docker registry.Pull Image from AWS registry:Update the YAML file to pull the image for our application.Docker VolumesDocker volumes provide a way to persist and share data between Docker containers and the host machine.In the above Golang app, whenever I was restarting or removing the container, it was deleting all the data from MongoDB.For stateful apps, we need docker volumes.We can define docker volumes in the YAML file and can associate container volumes with it.Define the docker volume in the YAML fileDocker volumes work by connecting or “plugging” the physical file system on the host machine to the file system inside the container. This allows data to be seamlessly shared and persisted between the host and the container.Volume TypesThere are 3 Volume types: Anonymous Volumes: Created and managed by Docker without a specific name assigned. Syntax: docker run -v /path/to/volume &amp;lt;image&amp;gt;2. Named Volumes: Explicitly created and named by the user for better management. Syntax: docker run -v &amp;lt;volume-name&amp;gt;:/path/to/volume &amp;lt;image&amp;gt;3. Bind Mounts: Maps a specific directory or file on the host to a directory in the container. Syntax: docker run -v /host/path:/container/path &amp;lt;image&amp;gt; Mostly used type is named volume. Unlike anonymous volumes, named volumes are explicitly created and named by the user, providing better control and management.Docker volume location:In Mac, the default folder is /var/lib/docker/volumeBut you won’t be able to see any folder here, the way it works on Mac is that docker for Mac creates a Linux VM and store data over there as shown below:Docker Volume path on the host machineIf you would like to view the full codebase, please visit the repository by clicking here: RepoReferences Video lecture by Techworld with NanaPost converted from Medium by ZMediumToMarkdown." }, { "title": "Understanding Mutex in Go", "url": "/posts/5f41199085b9/", "categories": "", "tags": "golang, mutex, concurrency, deadlock, defer", "date": "2023-07-07 19:27:52 +0000", "snippet": "Understanding Mutex in GoImage source: googleIntroductionIn concurrent programming, ensuring data integrity and preventing race conditions is crucial. In Go, the sync.Mutex type provides a simple a...", "content": "Understanding Mutex in GoImage source: googleIntroductionIn concurrent programming, ensuring data integrity and preventing race conditions is crucial. In Go, the sync.Mutex type provides a simple and effective way to achieve mutual exclusion and control concurrent access to shared resources. In this blog post, we will explore the concept of mutexes, understand how to use them in Go, and discuss their internals, and their role in solving race conditions.What is a Mutex and how it solves race conditions:A mutex, short for mutual exclusion, is used to protect shared resources from simultaneous access by multiple goroutines. It ensures that only one goroutine can access a critical section of code at a time. Race conditions occur when multiple goroutines access and modify shared data concurrently, leading to unpredictable and erroneous behavior. Mutexes prevent race conditions by allowing only one goroutine to acquire the lock and access the shared resource, while other goroutines wait until the lock is released.Mutexes are data structures provided by the standard sync package.Understanding Mutex InternalsLet’s try to understand how mutex prevents race conditions. It’s time to delve into the internals of mutex.Internally, the sync.Mutex type in Go utilizes low-level atomic operations provided by the underlying processor architecture. These atomic operations ensure that the mutex operations themselves are thread-safe and efficient.To understand the role of low-level atomic operations, let’s take a closer look at the implementation of the sync.Mutex type. While the exact implementation details may vary depending on the target platform, we can examine a simplified version that highlights the essential elements:package mainimport (\t\"fmt\"\t\"sync\")var counter = 0func increment(wg *sync.WaitGroup) {\tcounter++\twg.Done()}func main() {\tvar wg sync.WaitGroup\texpectedCounter := 1000\tfor i := 0; i &lt; expectedCounter; i++ {\t\twg.Add(1)\t\tgo increment(&amp;wg)\t}\twg.Wait()\tfmt.Println(\"Expected Counter:\", expectedCounter)\tfmt.Println(\"Actual Counter:\", counter)\t// Check for race condition\tif expectedCounter != counter {\t\tfmt.Println(\"Race condition detected!\")\t} else {\t\tfmt.Println(\"No race condition detected.\")\t}}In the simplified implementation above, the Mutex struct includes a state variable, which represents the lock’s state, and a sema variable, which is a semaphore used for blocking and waking up goroutines.The Lock() method attempts to acquire the lock by using the atomic.CompareAndSwapInt32() function. This function atomically compares the state variable’s value with 0 and swaps it with 1 if they are equal. If the swap is successful, the lock is acquired without blocking. Otherwise, the Lock() method calls runtime_SemacquireMutex() to wait for the lock to become available.The Unlock() method releases the lock by using the atomic.CompareAndSwapInt32() function again. It compares the state variable’s value with 1 and swaps it with 0 if they are equal. If the swap is successful, indicating that the lock was held, the Unlock() method calls runtime_SemreleaseMutex() to wake up any waiting goroutine.The use of atomic operations ensures that the lock’s state is updated atomically, without interference from other goroutines. This atomicity guarantees thread safety and eliminates the need for additional locks or synchronization mechanisms.How to prevent race conditionsIn Go, the sync package provides the Mutex type, which includes two main methods: Lock() and Unlock() .To understand how a mutex solves race conditions, let’s consider an example without using a mutex:package mainimport (\t\"fmt\"\t\"sync\")var counter = 0func increment(wg *sync.WaitGroup) {\tcounter++\twg.Done()}func main() {\tvar wg sync.WaitGroup\texpectedCounter := 1000\tfor i := 0; i &lt; expectedCounter; i++ {\t\twg.Add(1)\t\tgo increment(&amp;wg)\t}\twg.Wait()\tfmt.Println(\"Expected Counter:\", expectedCounter)\tfmt.Println(\"Actual Counter:\", counter)\t// Check for race condition\tif expectedCounter != counter {\t\tfmt.Println(\"Race condition detected!\")\t} else {\t\tfmt.Println(\"No race condition detected.\")\t}}Below is the output :OutputHere multiple goroutines are simultaneously reading and updating the value of the counter without any proper synchronization.Race conditionWe need to use the sync.Mutex type to prevent multiple goroutines from accessing counter at the same time:package mainimport (\t\"fmt\"\t\"sync\")var counter = 0var mutex sync.Mutexfunc increment(wg *sync.WaitGroup) {\tmutex.Lock()\tcounter++\tmutex.Unlock()\twg.Done()}func main() {\tvar wg sync.WaitGroup\texpectedCounter := 1000\tfor i := 0; i &lt; expectedCounter; i++ {\t\twg.Add(1)\t\tgo increment(&amp;wg)\t}\twg.Wait()\tfmt.Println(\"Expected Counter:\", expectedCounter)\tfmt.Println(\"Actual Counter:\", counter)\t// Check for race condition\tif expectedCounter != counter {\t\tfmt.Println(\"Race condition detected!\")\t} else {\t\tfmt.Println(\"No race condition detected.\")\t}}Below is the output:OutputNo race conditionsWhere Not to Use Mutex High Contention : If many goroutines are frequently trying for the same lock, the performance of mutexes can degrade. In such cases, consider using alternative synchronization primitives like sync.RWMutex or channel-based communication patterns. Deadlock Risks : Improper use of mutexes can lead to deadlocks, where goroutines end up waiting indefinitely for a lock to be released. Avoid complex nesting of locks or forgetting to unlock the mutex.Using defer with Unlock()It is very easy to miss unlocking the mutex. Whenever you call the Lock method, you must ensure that Unlock is eventually called, otherwise any goroutine trying to acquire the same lock will be blocked forever.package mainimport (\t\"fmt\"\t\"sync\")var mutex sync.Mutexfunc main() {\tvar wg sync.WaitGroup\twg.Add(2)\tgo func() {\t\tdefer wg.Done()\t\tmutex.Lock() // will wait here indefinitely if Goroutine 2 acquires lock first\t\tfmt.Println(\"Goroutine 1 acquired the lock\")\t\tn := 2\t\tif n%2 == 0 {\t\t\treturn\t\t}\t\tmutex.Unlock() // mutex is never unlocked\t}()\tgo func() {\t\tdefer wg.Done()\t\tmutex.Lock() // will wait here indefinitely if Goroutine 1 acquires lock first\t\tfmt.Println(\"Goroutine 2 acquired the lock\")\t\tn := 2\t\tif n%2 == 0 {\t\t\treturn\t\t}\t\tmutex.Unlock() // mutex is never unlocked\t}()\twg.Wait()\tfmt.Println(\"Main goroutine completed\")}In the above example, if either goroutine1 or goroutine2 acquires the lock, the unlocked goroutine will wait for the lock indefinitely. Here the if condition is always true and it will never unlock the mutex.outputWe can use defer here to prevent such kind of scenarios. Without defer, forgetting to manually release the lock before returning from a function can lead to deadlocks, where a goroutine may be blocked indefinitely.package mainimport (\t\"fmt\"\t\"sync\")var mutex sync.Mutexfunc main() {\tvar wg sync.WaitGroup\twg.Add(2)\tgo func() {\t\tdefer wg.Done()\t\tmutex.Lock()\t\tdefer mutex.Unlock()\t\tfmt.Println(\"Goroutine 1 acquired the lock\")\t\tn := 2\t\tif n%2 == 0 {\t\t\treturn\t\t}\t}()\tgo func() {\t\tdefer wg.Done()\t\tmutex.Lock()\t\tdefer mutex.Unlock()\t\tfmt.Println(\"Goroutine 2 acquired the lock\")\t\tn := 2\t\tif n%2 == 0 {\t\t\treturn\t\t}\t}()\twg.Wait()\tfmt.Println(\"Main goroutine completed\")}outputThe defer statement in Go allows us to postpone the execution of a function until the surrounding function returns. By deferring the Unlock() method immediately after acquiring the lock, we ensure that the mutex will always be released, even if an error occurs or a panic is triggered.ConclusionMutexes play a crucial role in concurrent programming to ensure proper synchronization and prevent race conditions. By using the sync.Mutex type in Go, developers can protect shared resources and control access to critical sections of code.If you would like to view the full codebase, please visit the repository by clicking here: RepoPost converted from Medium by ZMediumToMarkdown." }, { "title": "Exploring Context in Golang", "url": "/posts/34d9d3a5565b/", "categories": "Women, in, Technology", "tags": "", "date": "2023-07-06 15:05:16 +0000", "snippet": "Exploring Context in GolangConcurrency is an important part of Go programming, and managing goroutines effectively is vital for creating strong and scalable applications. The context package in Go ...", "content": "Exploring Context in GolangConcurrency is an important part of Go programming, and managing goroutines effectively is vital for creating strong and scalable applications. The context package in Go offers a useful tool for handling timeouts, cancellations, and sharing values specific to a request. In this blog post, we will explore the concept of context in Go, its significance, how to create and use contexts, and provide real-life examples that demonstrate how context can solve common challenges in concurrent programming.What is Context in go and why it is needed In Go, a context is a mechanism for managing concurrent operations, such as goroutines, by passing information and signals between different parts of a program. It allows for handling timeouts, cancellations, and sharing values in a controlled manner.For example, in a web server, a context can be created for each incoming request and passed to the corresponding goroutine. This context can carry request-specific data and control aspects like deadlines and cancellations.Let’s consider a scenario where you’re preparing a sandwich for a friend, and you’ve assigned a few people to buy the required ingredients like tomatoes and bread. However, your friend suddenly changes their mind and no longer wants the sandwich. In this case, you need to cancel all the ongoing operations related to buying the ingredients.The context also supports timeouts, ensuring that if an operation takes too long, it is automatically canceled. This helps prevent unnecessary delays and ensures efficient execution.Overall, a context in Go enables better management of concurrent operations by providing a structured way to share information, handle timeouts, and propagate cancellations. It enhances control and coordination in concurrent programming.Context CreationGo provides several functions to create contexts. When working with contexts in Go, there is a concept of a parent context and a child context. The parent context serves as the root context from which child contexts are derived.Let’s understand this concept in the context of the functions used to create contexts. context.Background() : This function returns a background context, which serves as the root parent context. It is often used as the starting point for creating other contexts. context.TODO() : The TODO function returns a context that is similar to Background() . It indicates that the specific context to use is not yet determined. It is typically used when a context is expected but not available at the moment. It is advisable to document the reason for using TODO and replace it with an appropriate context later. context.WithValue(parentContext, key, value) :This function creates a child context derived from a parent context ( parentContext ) . It associates a key-value pair with the child context, allowing for the passing of request-scoped values. The child context inherits the values from the parent context and can add or overwrite values specific to itself. context.WithTimeout(parentContext, timeout) : WithTimeout creates a child context derived from a parent context ( parentContext ) with a specified timeout duration. This child context is automatically canceled when the timeout duration elapses. The timeout value is specific to the child context and does not affect the parent context or other child contexts derived from the same parent. context.WithDeadline(parentContext, deadline) : WithDeadline creates a child context derived from a parent context ( parentContext ) with an explicit deadline. The child context is automatically canceled when the specified deadline is reached. Similar to WithTimeout , the deadline is specific to the child context and does not affect other contexts.Context InterfaceThe context package defines the Context interface, which represents a context in Go. The Context interface includes the following methods:// A Context carries a deadline, a cancellation signal, and other values across// API boundaries.//// Context's methods may be called by multiple goroutines simultaneously.type Context interface {\t// Deadline returns the time when work done on behalf of this context\t// should be canceled. Deadline returns ok==false when no deadline is\t// set. Successive calls to Deadline return the same results.\tDeadline() (deadline time.Time, ok bool)\t// Done returns a channel that's closed when work done on behalf of this\t// context should be canceled. Done may return nil if this context can\t// never be canceled. Successive calls to Done return the same value.\t// The close of the Done channel may happen asynchronously,\t// after the cancel function returns.\t// WithCancel arranges for Done to be closed when cancel is called;\t// WithDeadline arranges for Done to be closed when the deadline\t// expires; WithTimeout arranges for Done to be closed when the timeout\t// elapses.\t//\t// Done is provided for use in select statements\tDone() &lt;-chan struct{}\t// If Done is not yet closed, Err returns nil.\t// If Done is closed, Err returns a non-nil error explaining why:\t// Canceled if the context was canceled\t// or DeadlineExceeded if the context's deadline passed.\t// After Err returns a non-nil error, successive calls to Err return the same error.\tErr() error\t// Value returns the value associated with this context for key, or nil\t// if no value is associated with key. Successive calls to Value with\t// the same key returns the same result.\tValue(key any) any} Deadline() (deadline time.Time, ok bool) : Returns the context’s deadline, indicating when the associated operation should be completed. The ok flag indicates if a deadline is set. Done() &amp;lt;-chan struct{} : Returns a channel that is closed when the context is canceled or times out. Err() error : Returns the reason for context cancellation, which can be a timeout or a specific error value. Value(key interface{}) interface{} : Returns the value associated with a given key from the context. This allows for passing request-scoped values through the context.Context propogate from parent to childWhen a parent goroutine creates a child goroutine in Go, the context can be propagated from the parent to the child. Context propagation allows the child goroutine to inherit and carry forward the same context values, deadlines, and cancellations.In Go, context propagation is achieved through the use of the context.Context type, which is passed as an argument to functions or goroutines that need access to the context.Context HierarchyIn Go, context propagation is achieved through the use of the context.Context type, which is passed as an argument to functions or goroutines that need access to the context.Here’s a step-by-step explanation of how context propagates from a parent to a child goroutine: Parent Goroutine creates a Context: The parent goroutine creates a context.Context using one of the context creation functions, such as context.Background() , context.TODO() , or by deriving a new context from an existing context using functions like context.WithValue , context.WithTimeout , or context.WithDeadline . Parent Goroutine spawns a Child Goroutine: Once the context is created, the parent goroutine spawns a child goroutine using the go keyword or any other means of concurrent execution. The child goroutine is passed the context as an argument. Child Goroutine Receives the Context: In the child goroutine, the context passed from the parent goroutine is received as a parameter. This allows the child goroutine to access the same context and any associated values, deadlines, or cancellations. Context Operations in Child Goroutine: Inside the child goroutine, the context can be used to check for deadlines, retrieve values associated with keys using ctx.Value , or check for cancellation using ctx.Done() . Context Propagation in Subsequent Child Goroutines: If the child goroutine further spawns additional goroutines, the same context can be propagated by passing it to the newly created goroutines. This ensures that the context and its properties are available throughout the goroutine hierarchy.By propagating the context from parent to child goroutines, you ensure that important information, such as deadlines or cancellations, is carried forward and can be appropriately utilized in each goroutine.For instance, in above diagram canceling c2 will cancel c2, c4 and c5 only. It’s important to note that while the context itself is immutable, you can derive new contexts from existing ones with additional values, timeouts, or deadlines as needed. This allows each goroutine to have its own specific context while still inheriting the values and properties of its parent context.Real-Life Examples: WithValue ContextUser Authentication: Checking if a token is valid and setting an “authenticated” value in the context to represent the authentication status of a user.package mainimport (\t\"context\"\t\"fmt\"\t\"time\")func Authenticate(ctx context.Context, token string) bool {\tvalidToken := \"secret_token\"\tfmt.Println(\"-------request ID ------\", ctx.Value(\"requestID\"))\tctx = context.WithValue(ctx, \"authenticated\", false)\tif token == validToken {\t\tctx = context.WithValue(ctx, \"authenticated\", true)\t}\treturn ctx.Value(\"authenticated\").(bool)}func main() {\tctx := context.Background()\tctx = context.WithValue(ctx, \"requestID\", \"12345\")\tgo func(ctx context.Context) {\t\tisAuthenticated := Authenticate(ctx, \"secret_token\")\t\tfmt.Println(\"Authenticated:\", isAuthenticated)\t}(ctx)\tctx = context.WithValue(ctx, \"requestID\", \"12346\")\tgo func(ctx context.Context) {\t\tisAuthenticated := Authenticate(ctx, \"secret_token\")\t\tfmt.Println(\"Authenticated:\", isAuthenticated)\t}(ctx)\t// ...\t// Perform other concurrent operations\t// ...\ttime.Sleep(1 * time.Second)}outputAuthenticate function checks if a token is valid and sets the authenticated value in the context accordingly. The main function creates a background context using context.Background() and sets a request ID value. The Authenticate function is invoked as a goroutine, allowing concurrent authentication checks while using the same context across multiple goroutines.2. WithTimeout ContextExternal API Request : Making an API request with a timeout, where the request either completes within the specified timeout or gets canceled if it exceeds the timeout duration.package mainimport (\t\"context\"\t\"fmt\"\t\"time\")func makeAPIRequest(ctx context.Context) {\tselect {\tcase &lt;-ctx.Done():\t\tfmt.Println(\" API request cancelled \", ctx.Err())\tcase &lt;-time.After(3 * time.Second):\t\tfmt.Println(\"API request completed\")\t}}func main() {\tparentCtx := context.Background()\tchildCtx1, cancel := context.WithTimeout(parentCtx, 2*time.Second)\tdefer cancel()\tgo makeAPIRequest(childCtx1)\tchildCtx2, cancel := context.WithTimeout(parentCtx, 10*time.Second)\tdefer cancel()\tgo makeAPIRequest(childCtx2)\t// ...\t// Perform other concurrent operations\t// ...\ttime.Sleep(5 * time.Second)}Context HierarchyIn this example, the MakeAPIRequest function that simulates making an API request to an external endpoint. The function uses a select statement with a timeout of 3 seconds. If the request completes within the specified timeout, it prints a success message. Otherwise, if the context is canceled due to the timeout, it prints a cancellation message.Here childCtx1 will be cancelled as timeout is 2 seconds and childCtx2 will be completed in the given timeout.Output3. WithDeadline ContextTask Scheduling : Scheduling a task to be completed before a given deadline, where the task either completes within the deadline or gets canceled if it exceeds the deadline.package mainimport (\t\"context\"\t\"fmt\"\t\"time\")func ScheduleTask(ctx context.Context, taskName string) {\tselect {\tcase &lt;-time.After(4 * time.Second):\t\tfmt.Printf(\"Task '%s' completed\\n\", taskName)\tcase &lt;-ctx.Done():\t\tfmt.Printf(\"Task '%s' canceled: %v\\n\", taskName, ctx.Err())\t}}func main() {\tparentCtx := context.Background()\tdeadline := time.Now().Add(1 * time.Second)\tchildCtx1, cancel := context.WithDeadline(parentCtx, deadline)\tdefer cancel()\tgo ScheduleTask(childCtx1, \"Data Processing\")\tchildChildCtx1, cancel := context.WithDeadline(childCtx1, time.Now().Add(100*time.Second))\tdefer cancel()\tgo ScheduleTask(childChildCtx1, \"File Processing\")\tchildCtx2, cancel := context.WithDeadline(parentCtx, time.Now().Add(10*time.Second))\tdefer cancel()\tgo ScheduleTask(childCtx2, \"Some remote Processing\")\t// ...\t// Perform other concurrent operations\t// ...\ttime.Sleep(10 * time.Second)}Context HierarchyHere I am cancelling childCtx1(data processing ctx) but it is cancelling childchildCtx1 (file processing) as well even though deadline is 100 sec for that.outputConclusionContexts in Go provide a standardized way to manage goroutines, handle timeouts, cancellations, and share request-scoped values.If you would like to view the full codebase, please visit the repository by clicking here: RepoPost converted from Medium by ZMediumToMarkdown." }, { "title": "Exploring the Internals of Channels in Go", "url": "/posts/f01ac6e884dc/", "categories": "Women, in, Technology", "tags": "go, goroutines, backend-development, threading", "date": "2023-06-29 22:02:24 +0000", "snippet": "Exploring the Internals of Channels in GoIntroductionChannels are a vital component of concurrent programming in Go. They provide a safe and efficient way for goroutines to communicate and share in...", "content": "Exploring the Internals of Channels in GoIntroductionChannels are a vital component of concurrent programming in Go. They provide a safe and efficient way for goroutines to communicate and share information. Instead of directly sharing memory, Go promotes the use of channels for inter-goroutine communication. In this blog, we will delve into the internals of channels and explore how they work behind the scenes. So, let’s dive in and uncover the mysteries of Go channels! A goroutine is a lightweight thread managed by the Go runtime, enabling concurrent execution of functions or tasks in Go programs.Define ChannelTo define a channel in Go, you can use the syntax: var channelName chan ElementType . For example, var intChannel chan int creates an unbuffered channel for transmitting integers. If you want to create a buffered channel, use make(chan ElementType, bufferSize) it to specify the capacity of the channel.ch := make(chan string, 4) // buffered channelch := make(chan int) // unbuffered channelChannels in Go are designed to be goroutine-safe and follow the FIFO (First-In-First-Out) order. To meet these requirements, channels utilize a circular queue with a lock as their underlying implementation. The circular queue allows for efficient enqueueing and dequeuing of values, maintaining the order in which they were sent. The lock ensures that only one goroutine can access the channel at a time, preventing race conditions and ensuring synchronized access to the queue.So when we define a channel using the above syntax, the channel is created from the hchan struct, which has the following fields.The internal representation of buffered channel at runtimetype hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex}type waitq struct { first *sudog last *sudog}The hchan struct in Go channels holds several important fields that define the behavior and characteristics of the channel. Here’s a breakdown of these fields: qcount: It represents the number of items or data currently present in the channel’s queue. dataqsize: This field indicates the size of the circular queue. It is relevant for buffered channels and is the second parameter provided when creating a channel using the make function. elemsize: It denotes the size of a single element within the channel. buf: The buf field refers to the actual circular queue where data is stored in buffered channels. closed: This field indicates whether the channel is closed. It is initially set to 0 upon channel creation and is set to 1 when the channel is closed using the close( ) function. sendx and recvx: These fields track the current index in the buffer or circular queue. sendx increases when data is added to a buffered channel, while recvx increases when data is received from the channel. recvq and sendq: These fields represent the waiting queues for blocked goroutines that are either waiting to read data from or write data to the channel. It contains reference to another structure sudog, which also plays a role in channel operations but will be explored later in the blog. lock: The lock field is a mutex used to lock the channel during read or write operations, preventing multiple goroutines from accessing it simultaneously and avoiding potential deadlocks.Memory allocation of channelWhen a channel is created using the make function in Go, memory is allocated on the heap for the hchan struct, and the make function returns a pointer to that memory. As a result, we don’t need to pass a pointer to the channel during function calls since the channel itself is a pointer under the hood.package mainimport (\t\"fmt\"\t\"time\")func taskOne(task chan string) { // goroutine G1\tjob := &lt;-task\tfmt.Println(\"task One received the job\", job)}func taskTwo(task chan string) { // goroutine G2\tjob := &lt;-task\tfmt.Println(\"task Two received the job\", job)}func main() { // goroutine G\tch := make(chan string, 2)\tch &lt;- \"job1\"\tch &lt;- \"job2\"\tgo taskOne(ch)\tgo taskTwo(ch)\ttime.Sleep(1 * time.Second) // stops the main step to finish before other goroutines.}At executing line 20, where we are adding one job to channel, the hchan would be like :The sendx and recvx fields in the hchan struct point to the next element to be sent or received from the channel. They increment after each operation and are set to 0 when the queue is full or empty, respectively. For unbuffered channels in Go, the buf field in the hchan struct will be nil. Unbuffered channels do not have a queue or buffer to store values.Send and receive operations on buffered channelWhen a goroutine, such as G(main func in above code snippet), wants to write data to a buffered channel, it follows these steps: To ensure safe modification of the channel and the underlying hchan struct, G (the goroutine) acquires a lock before writing data. This lock prevents concurrent access and maintains synchronization. After acquiring the lock, G performs an enqueue operation on the circular queue represented by the buf field. Before enqueuing the data, a memory copy operation is performed to create a copy of the data. Once the enqueue operation is completed, G1 releases the lock, allowing other goroutines to acquire it and perform their respective operations.When a goroutine, like G1(taskOne) or G2(taskTwo), reads data from the channel, it goes through similar steps as G but with some variations: G2 acquires the lock to ensure exclusive access to the channel’s hchan struct. It performs a dequeue operation on the circular queue (buf) to retrieve the next available data. At the same time, G2 performs a memory copy operation on the data it receives, creating a copy. Once G2 has copied the data from the buffer, it releases the lock , allowing other goroutines to access the channel. G2 can now process the copied data as needed, independently of other goroutines. It’s important to note that the data obtained by G2 is a separate copy, not a shared reference. This means that each goroutine receives its own copy of the data, ensuring data isolation and avoiding issues related to shared memory access.Buffer Overflow/UnderflowWhen the buffer capacity of a channel is reached and a goroutine, such as G, attempts to write data, the behavior depends on whether there is a receiver ready to receive the data.If there is a receiver (e.g., G2) ready to receive the data, G can proceed to send the data without blocking. The data is then received by G2, and both goroutines continue their execution.However, if there is no receiver ready to receive the data, G is paused. G will remain in a paused state, waiting for a receiver to become available. How does this pausing and resuming of goroutine works?Go runtime schedular does the magic here.Blocking call on buffered channelGo runtime SchedularBefore diving into schedular, let’s understand a bit about goroutines. As you might already be aware, goroutines in Go are user-space threads that are managed by the Go runtime scheduler. Unlike operating system threads, the lifecycle of goroutines is managed by the Go runtime rather than the operating system itself. This distinction makes goroutines lightweight compared to OS threads, resulting in lower resource consumption and reduced scheduling overhead.The Go runtime scheduler employs an M:N scheduling model , where M represents the number of goroutines and N represents the number of operating system threads. The scheduler multiplexes or maps these M goroutines onto the available N OS threads. This allows the scheduler to efficiently schedule and switch between goroutines, providing concurrent execution and parallelism on top of the underlying operating system threads.By utilizing the M:N scheduling model, the Go runtime scheduler achieves a balance between efficient resource utilization and effective concurrency management. Goroutines can be created and executed with low overhead, allowing developers to utilize concurrent programming in a lightweight and efficient manner.Image source : GoogleGo schedular has three structures : M represents the OS thread, which is managed by the operating system itself. G represents the goroutine, which is a resizable stack. P represents a context for scheduling and is responsible for running the Go code. It contains Queue of runnable goroutines.There must be association between os thread(M) and goroutine(G) for it to be running. The association between an OS thread and a goroutine is dynamic and can change over time.Since go runtime schedular is pretty much clear now, it’s time to move back to previous example. Goroutine G tries to send data to channel which is already full.ch &lt;- \"Job3\" // Goroutine G sending Job3 on a full channel2. It calls runtime schedular ( gopark function)3. Schedular changes G to waiting state and remove the association between OS thread (m) and Goroutine (g) .4. Schedular pops the goroutine from runQueue(p) and schedule it to run on OS thread (m) . This is context switching. G is blocked but not the OS thread.Here sudog comes into the picture. The sudog struct mentioned below is responsible for storing information about a waiting goroutine, such as g in our case. The Go runtime will park or suspend this sending goroutine ( g ), ensuring that it is temporarily halted until certain conditions are met.call ch &lt;- “Job3”(blocking send) creates a sudog and add it to waiting sendertype sudog struct { // The following fields are protected by the hchan.lock of the // channel this sudog is blocking on. shrinkstack depends on // this for sudogs involved in channel ops. g *g next *sudog prev *sudog elem unsafe.Pointer // data element (may point to stack) // The following fields are never accessed concurrently. // For channels, waitlink is only accessed by g. // For semaphores, all fields (including the ones above) // are only accessed when holding a semaRoot lock. acquiretime int64 releasetime int64 ticket uint32 // isSelect indicates g is participating in a select, so // g.selectDone must be CAS'd to win the wake-up race. isSelect bool // success indicates whether communication over channel c // succeeded. It is true if the goroutine was awoken because a // value was delivered over channel c, and false if awoken // because c was closed. success bool parent *sudog // semaRoot binary tree waitlink *sudog // g.waiting list or semaRoot waittail *sudog // semaRoot c *hchan // channel}Now see what happens when goroutine G1 or G2 are scheduled by runtime schedular and they perform receive operation on same channel. The G1/G2 dequeues an object (JOB1) from its buffer, effectively receiving task from queue. It assigns JOB1 to the variable job. Additionally, it dequeues the sudog from the sendq (send queue) and enqueues the sudog.elem (“JOB3”) into the buffer. It is a performance optimisation here. It saves few memory operations.job := &lt;-ch // Goroutine G1/G2 receive data from on a buffered channel3. Call goready function to move G(main func) to runQueue and make it runnable.Current state of hchan when G1/G2 receives data from buffered channel which was blocked earlier What happens when receive comes first and channel is empty?Let’s say channel is empty and Goroutine G1 tries to read datajob := &lt;-ch // Goroutine G1 try to read data from on an empty channelG1 is temporarily suspended and will remain paused until it is awakened by a subsequent send operation on the channel. G1 creates a sudog and puts it in the receq (receive queue), It calls gopark(G1) to pause the execution of the goroutine.Now G(main goroutine) gets the schedular and there are two possibilities. Enqueue the task in the buffer and call goready(G1) : In this case, G would put the task in the channel’s buffer and then call goready(G1) to make G1 runnable again. This approach involves acquiring the lock and performing additional memory operations. Directly copy the task to the elem field of the sudog of G1: Instead of enqueuing the task into the channel’s buffer, G directly copies the task to the elem field of G1’s sudog . This approach avoids the need to acquire the lock and reduces the number of memory operations required.The Go scheduler opts for the second option as a performance optimization. By directly copying the task, it minimizes the overhead associated with acquiring locks and reduces memory operations, resulting in improved performance and efficiency.Send/receive in unbuffered channelsIn unbuffered channels, the send and receive operations work differently depending on the order of execution: When a receive operation occurs first, the sender directly writes the value to the receiver’s stack. This means that the value is transferred directly from the sender to the receiver without any intermediate storage or buffering. Conversely, when a send operation occurs first, the receiver receives the value directly from the sudog (synchronization data structure) of the sender. The value is obtained without the need for buffering or additional intermediate steps.This direct transfer of data between the sender and receiver in unbuffered channels eliminates the need for a separate buffer, ensuring that the send and receive operations are tightly synchronized. The direct transfer mechanism enables efficient and synchronous communication between goroutines, facilitating a strict one-to-one data exchange pattern.Post converted from Medium by ZMediumToMarkdown." }, { "title": "Protocol Buffers (ProtoBuf) in Go", "url": "/posts/93eb6138f1c0/", "categories": "", "tags": "protobuf, golang, programming, golang-tutorial", "date": "2023-05-20 18:12:39 +0000", "snippet": "Protocol Buffers (ProtoBuf) in GoBuckle up, folks! This time we’re diving into the world of Protocol Buffers (protobuf) and their superpowers in data serialization.IntroductionProtocol Buffers, als...", "content": "Protocol Buffers (ProtoBuf) in GoBuckle up, folks! This time we’re diving into the world of Protocol Buffers (protobuf) and their superpowers in data serialization.IntroductionProtocol Buffers, also referred to as protobuf, is a language-agnostic binary serialization format that has been developed by Google. Its primary purpose is to efficiently serialize structured data for inter-system communication and data storage.[Image source: Google]Major benefits of protobuf : Compactness : Protobuf provides efficient serialization, resulting in smaller message sizes for improved bandwidth usage. Schema Evolution : Protobuf supports schema evolution without breaking compatibility, allowing seamless updates to data structures. Efficient Serialization and Deserialization : Protobuf offers fast and efficient serialization, improving overall system performance. Cross-Platform Support : Protobuf allows seamless data exchange across different platforms and languages.These benefits make protobuf a powerful tool for efficient data communication and storage in Go applications.How It Is Better Than JSON and XML:[Image source: Google]XML, known as Extensible Markup Language, is like a map that helps organize and structure data using tags. It presents information in a way that both humans and machines can understand. However, XML can be wordy and take up more space, which may slow down performance and make data transmission less efficient.JSON, or JavaScript Object Notation, is like a messenger that uses a simple key-value structure to represent data objects. It has become popular for transmitting data between web services because it is easy to read and work with. But JSON’s text-based format can result in larger file sizes, which can affect the speed of data transfer.In contrast, Protocol Buffers (protobuf) shine in the world of data serialization. It’s like a magic trick that transforms data into a compact and efficient binary format. Protobuf is known for its fast data processing and the ability to adapt to changing data structures without breaking compatibility. It can be used with different programming languages and ensures the reliability of your data.In summary, XML and JSON have their uses, but if you need a powerful and efficient data serialization solution, Protocol Buffer (protobuf) is the way to go. It provides compactness, speed, flexibility, and compatibility, making it a top choice for handling data efficiently.The Serialization Performance: Protocol Buffers vs. JSON in Golang Enough Talk, Let’s Get Our Hands Dirty[Image source: Google] Visit the official Protocol Buffers GitHub repository ( https://github.com/protocolbuffers/protobuf ) to download the compiler compatible with your operating system.2. Define a protobuf message schema using the .proto file format.```protocol buffersyntax = “proto3”;package main;option go_package = “/;msgmodel”;message MyMessage { int32 id = 1; string name = 2; string email = 3;}3\\. Compile the file&gt; protoc — go\\_out=\\. \\./\\*proto This command generates Go code bindings from the protobuf schema\\. The `--go_out` flag specifies that the output should be in Go\\. This will generate a msg\\.pb\\.go file, that contains the necessary code bindings for your protobuf schema\\.4\\. Implement a benchmark test in Golang that serializes a large dataset using both protobuf and JSON:```gopackage mainimport (\t\"encoding/json\"\t\"github.com/golang/protobuf/proto\"\t\"go-protobuf/model/message\"\t\"log\"\t\"testing\")const (\titeration = 10000000 //Number of iterations for the benchmark test)func generateDataset() []*message.MyMessage {\tvar dataset []*message.MyMessage\tfor i := 0; i &lt; iteration; i++ {\t\tdata := &amp;message.MyMessage{\t\t\tEmail: \"johndoe@example.com\",\t\t\tName: \"John Doe\",\t\t\tId: int32(i),\t\t}\t\tdataset = append(dataset, data)\t}\treturn dataset}func BenchmarkProtobufSerialisation(b *testing.B) {\tdataset := generateDataset()\tb.ResetTimer()\tfor n := 0; n &lt; b.N; n++ {\t\tfor _, data := range dataset {\t\t\t_, err := proto.Marshal(data)\t\t\tif err != nil {\t\t\t\tlog.Fatal(err)\t\t\t}\t\t}\t}}func BenchmarkJSONSerialization(b *testing.B) {\tdataset := generateDataset()\tb.ResetTimer()\tfor n := 0; n &lt; b.N; n++ {\t\tfor _, data := range dataset {\t\t\t_, err := json.Marshal(data)\t\t\tif err != nil {\t\t\t\tlog.Fatal(err)\t\t\t}\t\t}\t}}func main() {\t// Run the benchmark tests\ttesting.Benchmark(BenchmarkProtobufSerialisation)\ttesting.Benchmark(BenchmarkJSONSerialization)}5. Based on the benchmark results (shown below), it is evident that Protobuf outperforms JSON serialization in terms of speed. The protobuf serialization benchmark was completed in significantly less time compared to the JSON serialization benchmark.Memory Performance Comparison: JSON vs. Protocol Buffers Implement a benchmark test in Golang that compares memory usage of a large dataset using both protobuf and JSON:package mainimport (\t\"encoding/json\"\t\"github.com/golang/protobuf/proto\"\t\"go-protobuf/model/message\"\t\"log\"\t\"runtime\"\t\"runtime/debug\"\t\"testing\")const (\titeration = 100000000 //Number of iterations for the benchmark test)func generateDataset() []*message.MyMessage {\tvar dataset []*message.MyMessage\tfor i := 0; i &lt; iteration; i++ {\t\tdata := &amp;message.MyMessage{\t\t\tEmail: \"johndoe@example.com\",\t\t\tName: \"John Doe\",\t\t\tId: int32(i),\t\t}\t\tdataset = append(dataset, data)\t}\treturn dataset}func BenchmarkProtobufSerialisation(b *testing.B) {\tdataset := generateDataset()\tb.ResetTimer()\tfor n := 0; n &lt; b.N; n++ {\t\tfor _, data := range dataset {\t\t\t_, err := proto.Marshal(data)\t\t\tif err != nil {\t\t\t\tlog.Fatal(err)\t\t\t}\t\t}\t}\tmeasureMemoryUsage(b)}func BenchmarkJSONSerialization(b *testing.B) {\tdataset := generateDataset()\tb.ResetTimer()\tfor n := 0; n &lt; b.N; n++ {\t\tfor _, data := range dataset {\t\t\t_, err := json.Marshal(data)\t\t\tif err != nil {\t\t\t\tlog.Fatal(err)\t\t\t}\t\t}\t}\tmeasureMemoryUsage(b)}func measureMemoryUsage(b *testing.B) {\tdebug.FreeOSMemory()\tvar mem runtime.MemStats\truntime.GC()\truntime.ReadMemStats(&amp;mem)\tb.ReportMetric(float64(mem.Alloc)/1024/1024, \"Memory_MB\")}func main() {\t// Run the benchmark tests\ttesting.Benchmark(BenchmarkProtobufSerialisation)\ttesting.Benchmark(BenchmarkJSONSerialization)}2. The benchmark results show that JSON serialization used more memory compared to Protobuf serialization. On average, JSON serialization consumed around 0.2052 MB of memory, while protobuf serialization used only about 0.2042 MB. Although the difference is small, it’s clear that protobuf is more efficient in terms of memory usage. This means that protobuf’s compact binary format helps save memory, making it a good choice for working with large datasets and improving performance.ConclusionIt’s conclusion time now ! ! !Protocol Buffers (protobuf) have demonstrated superior performance and memory efficiency compared to JSON serialization in Golang. With its compact binary format and efficient serialization mechanism, protobuf offers smaller message sizes, improved network efficiency, and reduced bandwidth usage. Additionally, its schema evolution capabilities allow for seamless updates to data models. While JSON has its strengths, protobuf excels in scenarios that demand high-speed and memory-efficient data serialization, enabling optimized data transmission and improved system performance.[Image Source: Google]The complete codebase for the examples discussed in this article is available on the Git repository .Post converted from Medium by ZMediumToMarkdown." }, { "title": "Exploring Design Patterns in Go", "url": "/posts/fe2d7a895ab3/", "categories": "", "tags": "golang, design-patterns, backend, backend-development", "date": "2023-05-17 18:18:51 +0000", "snippet": "Exploring Design Patterns in GoDesign patterns are reusable solutions to common software design problems that help developers build software that is maintainable, extensible, and scalable. In this ...", "content": "Exploring Design Patterns in GoDesign patterns are reusable solutions to common software design problems that help developers build software that is maintainable, extensible, and scalable. In this article, we will explore some popular design patterns in Go: Builder, Decorator, Factory Method, Fan-in-out, and Singleton.1. Builder PatternThe Builder pattern separates the construction of a complex object from its representation, allowing the same construction process to create different representations. It provides a step-by-step approach to building objects. Let’s see an example of the Builder pattern in Go:type User struct {\tName string\tRole string\tSalary int}type UserBuilder struct {\tUser}func (ub *UserBuilder) setName(name string) *UserBuilder {\tub.User.Name = name\treturn ub}func (ub *UserBuilder) setRole(role string) *UserBuilder {\tub.User.Role = role\treturn ub}func (ub *UserBuilder) setSalary(sal int) *UserBuilder {\tub.User.Salary = sal\treturn ub}func (ub *UserBuilder) Build() User {\treturn ub.User}func main() {\tub := &amp;UserBuilder{}\tuser := ub.\t\tsetName(\"John Doe\").\t\tsetRole(\"Admin\").\t\tsetSalary(5000).\t\tBuild()}In the above example, UserBuilder provides methods to set different attributes of the user and a Build method to create the final User object.2. Decorator PatternThe Decorator pattern allows you to wrap existing functionality and append or prepend your own custom functionality on top. It enables adding behavior to objects dynamically at runtime. Let’s see an example of the Decorator pattern in Go:/*Decorators essentially allow you to wrap existing functionality and append or prependyour own custom functionality on top.*/func mainFun() {\tfmt.Println(\"main func\")\ttime.Sleep(1 * time.Second)}func additionalFun(a func()) {\tfmt.Printf(\"Starting function execution: %s\\n\", time.Now())\ta()\tfmt.Printf(\"ending function execution: %s\\n\", time.Now())}func main() {\tadditionalFun(mainFun)}In this example, we have a mainFun function representing the main functionality, and the additionalFun function acts as a decorator, adding additional behavior before and after executing the mainFun function by accepting it as an argument.3. Factory MethodThe Factory Method pattern allows the creation of objects without specifying their exact types and delegates the instantiation to the factory. It provides a way to decouple the abstraction and implementation of object creation and enables flexibility in creating different types of objects.type Engine interface{\tStart()\tStop()}type car struct{}func(c car) Start(){\tfmt.Println(\"car start\")}func(c car) Stop(){\tfmt.Println(\"car stop\")}type train struct{}func(c train) Start(){\tfmt.Println(\"train start\")}func(c train) Stop(){\tfmt.Println(\"train stop\")}func starting( e Engine){\te.Start()}func stopping( e Engine){\te.Stop()}func GetEngine(engineType string) Engine {\tswitch engineType {\tcase \"car\":\t\treturn car{}\tcase \"train\":\t\treturn train{}\tdefault:\t\tfmt.Println(\"type undefined\")\t\treturn nil\t}}func main(){\tengine := GetEngine(\"car\")\tstarting(engine)\tstopping(engine)\tengine1 := GetEngine(\"train\")\tstarting(engine1)\tstopping(engine1)}In this example, we have a factory method called GetEngine that returns Car or Train objects based on the specified engineType. The main function showcases the usage of the factory method by creating engine objects and invoking their Start( ) and Stop( ) methods.4. Fan-In and Fan-Out Patterns Fan-In PatternIn the Fan-In pattern, multiple input channels are combined into a single output channel. The inputs can come from different sources, and the Fan-In pattern allows you to merge and process the data from these sources concurrently. It aggregates data from multiple channels into a single channel, enabling centralized processing. Here’s an example:package mainimport (\t\"fmt\"\t\"sync\")func generator(start, end int) &lt;-chan int {\tch := make(chan int)\tgo func() {\t\tfor i := start; i &lt;= end; i++ {\t\t\tch &lt;- i\t\t}\t\tclose(ch)\t}()\treturn ch}func squareWorker(in &lt;-chan int, out chan&lt;- int, wg *sync.WaitGroup) {\tdefer wg.Done()\tfor num := range in {\t\tresult := num * num // Square the input\t\tout &lt;- result\t}}func main() {\tnumbers := generator(1, 5)\tsquaredNumbers := make(chan int)\tvar wg sync.WaitGroup\twg.Add(3)\tfor i := 0; i &lt; 3; i++ {\t\tgo squareWorker(numbers, squaredNumbers, &amp;wg)\t}\tgo func() {\t\twg.Wait()\t\tclose(squaredNumbers)\t}()\tfor res := range squaredNumbers {\t\tfmt.Println(res)\t}}In this Fan-In example, we have a generator function that returns a channel that produces numbers in a specified range.We have multiple squareWorker goroutines that receive numbers from the numbers channel, square each number, and send the squared result to the squaredNumbers channel. Each worker goroutine is synchronized using a sync.WaitGroup .The main goroutine uses the generator function to create a numbers channel with numbers from 1 to 5. It then launches multiple squareWorker goroutines to process the numbers concurrently.The main goroutine waits for the workers to finish processing by calling wg.Wait() . It then closes the squaredNumbers channel and receives the squared numbers from the channel, printing them.2. Fan-out PatternIn the Fan-Out pattern, a single input channel is divided and distributed among multiple worker goroutines. Each worker receives a portion of the workload and operates on it independently. The Fan-Out pattern allows you to parallelize the processing of data by dividing the workload and assigning it to multiple workers.package mainimport (\t\"fmt\"\t\"sync\")func producer(ch chan&lt;- int) {\tfor i := 1; i &lt;= 5; i++ {\t\tch &lt;- i\t}\tclose(ch)}func worker(in &lt;-chan int, out chan&lt;- int, wg *sync.WaitGroup) {\tdefer wg.Done()\tfor num := range in {\t\tresult := num * 2 // Process the input\t\tout &lt;- result\t}}func main() {\tinput := make(chan int)\toutput := make(chan int)\tgo producer(input)\tvar wg sync.WaitGroup\tworkerCount := 3\twg.Add(workerCount)\tfor i := 0; i &lt; workerCount; i++ {\t\tgo worker(input, output, &amp;wg)\t}\tgo func() {\t\twg.Wait()\t\tclose(output)\t}()\tfor res := range output {\t\tfmt.Println(res)\t}}In this Fan-Out example, we have a producer goroutine that sends numbers from 1 to 5 to the input channel and then close it.We also have multiple worker goroutines that receive numbers from the input channel, perform some processing (in this case, multiply the number by 2), and send the result to the output channel. Each worker is synchronized using a sync.WaitGroup .The main goroutine waits for the workers to finish processing by calling wg.Wait() . It then closes the output channel and receives the processed results from the output channel, printing them.5. Singelton PatternThe Singleton pattern ensures that only one instance of a class is created throughout the application. It provides a global point of access to this instance, allowing shared access to its resources. Let’s explore different implementations of the Singleton pattern in Go:1. Not Thread Safe (NTS)The first implementation we’ll look at is the Non-Thread Safe (NTS) approach. While simple, it’s not suitable for concurrent scenarios:type singleton struct {\tval int}var instance *singleton/*1) Not thread safe(NTS)*/func GetSingletonNTS() *singleton {\tif instance == nil {\t\tinstance = &amp;singleton{}\t}\treturn instance}In this implementation, the GetSingletonNTS function lazily initializes the instance variable. However, if multiple goroutines access GetSingletonNTS simultaneously and instance is still nil , they may end up creating separate instances.2. Mutex LockTo introduce thread safety, we can use a sync.Mutex to control access to the initialization code:type singleton struct {\tval int}var instance *singletonvar lock sync.Mutexfunc GetSingletonML() *singleton {\tlock.Lock()\tdefer lock.Unlock()\tif instance == nil {\t\tinstance = &amp;singleton{}\t}\treturn instance}In this implementation, we use a sync.Mutex named lock to synchronize access to the instance initialization. While it ensures thread safety, it introduces a potential bottleneck when multiple goroutines need to acquire the lock.3. Check-Lock-CheckWe can improve performance by minimizing the number of lock acquisitions using a double-checked locking approach:type singleton struct {\tval int}var instance *singletonvar lock sync.Mutexfunc GetSingletonCLC() *singleton {\tif instance == nil {\t\tlock.Lock()\t\tdefer lock.Unlock()\t\tif instance == nil {\t\t\tinstance = &amp;singleton{}\t\t}\t}\treturn instance}In this implementation, we first perform a quick check on instance without acquiring the lock. If instance is still nil , we acquire the lock and perform a second check before creating the instance. This approach reduces the number of lock acquisitions but is prone to subtle bugs in certain scenarios.4. Using sync.OnceThe recommended way to implement the Singleton pattern in Go is to use the sync.Once package’s Do function:type singleton struct {\tval int}var instance *singletonvar once sync.Oncefunc GetSingletonOnceDo() *singleton {\tonce.Do(func() {\t\tinstance = &amp;singleton{}\t})\treturn instance}In this implementation, the GetSingletonOnceDo function ensures that the initialization code inside the once.Do block is executed only once, regardless of the number of calls to GetSingletonOnceDo . It provides thread safety and better performance compared to the other implementations.The complete codebase for the examples discussed in this article is available on the Git repository, allowing you to easily access and experiment with the entire codebase.Post converted from Medium by ZMediumToMarkdown." }, { "title": "Demystifying Load Balancing in Go: A Comprehensive Guide", "url": "/posts/678dd07129a2/", "categories": "", "tags": "golang, load-balancing, load-balancer, tech, backend", "date": "2023-05-16 16:56:41 +0000", "snippet": "Demystifying Load Balancing in Go: A Comprehensive GuideLoad balancing is a crucial technique used to distribute incoming traffic across multiple backend servers, ensuring optimal performance, scal...", "content": "Demystifying Load Balancing in Go: A Comprehensive GuideLoad balancing is a crucial technique used to distribute incoming traffic across multiple backend servers, ensuring optimal performance, scalability, and reliability. In this article, we will explore load balancing in Go and implement three popular load balancing algorithms: Round Robin, Least Connections, and Random. We will also discuss the differences between layer 4 and layer 7 load balancing.Table of Contents: Load Balancing Fundamentals Layer 4 vs. Layer 7 Load Balancing Round Robin Load Balancer Least Connections Load Balancer Random Load Balancer Implementation in Go Conclusion1. Load Balancing FundamentalsLoad balancing involves distributing incoming requests across multiple backend servers to achieve better performance, scalability, and fault tolerance. It ensures that no single server is overwhelmed with traffic while maintaining high availability and efficient resource utilization.There are two primary types of load balancing:Layer 4 Load Balancing:Layer 4 load balancing operates at the transport layer (TCP/UDP) of the OSI model. It focuses on distributing traffic based on IP addresses, ports, and transport protocols. Layer 4 load balancers make routing decisions based on network-level information without inspecting application-layer protocols.Layer 7 Load Balancing:Layer 7 load balancing, also known as application-level load balancing, operates at the application layer of the OSI model. It performs deep packet inspection, allowing load balancers to make routing decisions based on the content, URL, cookies, or other application-specific data. Layer 7 load balancers are more intelligent and can distribute traffic based on application-specific needs.3. Round Robin Load Balancer:The Round Robin load balancing algorithm distributes requests in a sequential, circular manner. Each request is assigned to the next available backend server in the rotation. It ensures an even distribution of requests across all servers. However, Round Robin does not consider server loads or capacities, potentially leading to uneven resource utilization.4. Least Connections Load BalancerThe Least Connections load balancing algorithm distributes requests to the backend server with the fewest active connections. It aims to evenly distribute the workload by considering the current connections on each server. This algorithm ensures that traffic is directed to servers with lighter loads, enabling better utilization of resources.5. Random Load BalancerThe Random load balancing algorithm randomly selects a backend server for each incoming request. It is the simplest load-balancing algorithm but does not consider server loads or capacities. Random load balancing can be effective when backend servers have similar capabilities, but it may lead to uneven distribution if servers have varying capacities or loads.6. Implementation in GoLoadBalancer the interface specifies the ServeHTTP method and the GetNextAvailableServer method. The Server struct represents a backend server, including its URL, health status, weight, and current number of connections.// LoadBalancer defines the interface for a load balancer.type LoadBalancer interface {\tServeHttp(w http.ResponseWriter, r *http.Request)\tGetNextAvailableServer() *Server}// Server represents a backend server.type Server struct {\tURL string\tAlive bool\tWeight int\tConnections int\tmutex sync.Mutex // using it to protect concurrent access to alive and connections field}The ReverseProxy struct represents a reverse proxy for a specific backend server. The NewReverseProxy function creates a new instance of the ReverseProxy struct with the specified backend URL. The ServeHTTP method of the ReverseProxy struct forwards the incoming request to the backend server.type ReverseProxy struct {\tbackendURL string\tproxy *httputil.ReverseProxy}func NewReverseProxy(backendURL string) *ReverseProxy {\tbackend, _ := url.Parse(backendURL)\treturn &amp;ReverseProxy{\t\tbackendURL: backendURL,\t\tproxy: httputil.NewSingleHostReverseProxy(backend),\t}}// Forwards the incoming request to backend serverfunc (rp *ReverseProxy) ServerHttp(w http.ResponseWriter, r *http.Request) {\tfmt.Printf(\"Forwarding request to %s : %s\\n\", rp.backendURL, r.URL.Path)\trp.proxy.ServeHTTP(w, r)}The RoundRobinLoadBalancer struct maintains a list of servers and keeps track of the next server to use for load balancing.The ServeHTTP method of RoundRobinLoadBalancer distributes incoming requests in a round-robin manner. It calls the GetNextAvailableServer method to obtain the next available server and creates a reverse proxy for that server’s URL. The reverse proxy then forwards the request to the backend server.package mainimport (\t\"net/http\")type RoundRobinLB struct {\tservers []*Server\tnext int}func NewRoundRobinLB(servers []*Server) *RoundRobinLB {\treturn &amp;RoundRobinLB{\t\tservers: servers,\t\tnext: 0,\t}}// GetNextAvailableServer returns the next available backend server in a round-robin manner.func (lb *RoundRobinLB) GetNextAvailableServer() *Server {\tnumServers := len(lb.servers)\t// Start searching from the next index\tstart := lb.next\tfor i := 0; i &lt; numServers; i++ {\t\tserverIndex := (start + i) % numServers\t\tserver := lb.servers[serverIndex]\t\tserver.mutex.Lock()\t\talive := server.Alive\t\tserver.mutex.Unlock()\t\tif alive {\t\t\t// update the next index for next iteration\t\t\tlb.next = (serverIndex + 1) % numServers\t\t\treturn server\t\t}\t}\t// No available servers found, return nil\treturn nil}func (lb *RoundRobinLB) ServeHTTP(w http.ResponseWriter, r *http.Request) {\tserver := lb.GetNextAvailableServer()\tif server != nil {\t\tproxy := NewReverseProxy(server.URL)\t\tlogger.Print(\"server is \", server)\t\t// Set the logger for the ReverseProxy\t\tproxy.ServerHttp(w, r)\t} else {\t\tlogger.Print(\"server is \", server)\t\t// TODO:\t\t// Handle the case when no available server is found\t}}The LeastConnectionsLoadBalancer struct represents a load balancer that distributes incoming requests to the backend server with the fewest active connections. The NewLeastConnectionsLoadBalancer function creates a new instance of the LeastConnectionsLoadBalancer struct with the given list of servers.The ServeHTTP method of LeastConnectionsLoadBalancer distributes incoming requests to the backend server with the fewest active connections. It calls the GetNextAvailableServer method to obtain the server with the fewest connections and creates a reverse proxy for that server’s URL. The reverse proxy then forwards the request to the backend server.package mainimport (\t\"net/http\")type LeastConnectionLB struct {\tservers []*Server}func NewLeastConnectionLB(servers []*Server) *LeastConnectionLB {\treturn &amp;LeastConnectionLB{\t\tservers: servers,\t}}// ServeHTTP distributes the incoming request to the backend server with the fewest active connections.func (lb *LeastConnectionLB) ServeHTTP(w http.ResponseWriter, r *http.Request) {\tserver := lb.GetNextAvailableServer()\tlogger.Print(\"server is %v\", server)\tproxy := NewReverseProxy(server.URL)\tproxy.ServerHttp(w, r)}// GetNextAvailableServer returns the backend server with the fewest active connections.func (lb *LeastConnectionLB) GetNextAvailableServer() *Server {\tminConn := -1\tvar selectedServer *Server\tfor _, server := range lb.servers {\t\tserver.mutex.Lock()\t\talive := server.Alive\t\tconnections := server.Connections\t\tserver.mutex.Unlock()\t\tif !alive {\t\t\tcontinue\t\t}\t\tif minConn == -1 || connections &lt; minConn {\t\t\tminConn = connections\t\t\tselectedServer = server\t\t}\t}\tif selectedServer != nil {\t\tselectedServer.mutex.Lock()\t\tselectedServer.Connections++\t\tselectedServer.mutex.Unlock()\t\treturn selectedServer\t}\t// No available servers found, return nil\treturn nil}The RandomLoadBalancer struct represents a load balancer that distributes incoming requests to a random backend server. The NewRandomLoadBalancer function creates a new instance of the RandomLoadBalancer struct with the given list of servers.The ServeHTTP method of RandomLoadBalancer distributes incoming requests to a random backend server. It calls the GetNextAvailableServer method to obtain a random available server and creates a reverse proxy for that server’s URL. The reverse proxy then forwards the request to the backend server.package mainimport (\t\"math/rand\"\t\"net/http\")type RandomLB struct {\tservers []*Server}func NewRandomLB(servers []*Server) *RandomLB {\treturn &amp;RandomLB{\t\tservers: servers,\t}}// ServeHTTP distributes the incoming request to a random backend server.func (lb *RandomLB) ServeHTTP(w http.ResponseWriter, r *http.Request) {\tserver := lb.GetNextAvailableServer()\tif server != nil {\t\tproxy := NewReverseProxy(server.URL)\t\tlogger.Print(\"server is \", server)\t\tproxy.ServerHttp(w, r)\t} else {\t\tlogger.Print(\"server is \", server)\t\t//TODO:\t\t// Handle the case when no available server is found\t}}// GetNextAvailableServer returns a random backend server.func (lb *RandomLB) GetNextAvailableServer() *Server {\tvar availableServers []*Server\tfor _, server := range lb.servers {\t\tserver.mutex.Lock()\t\tif server.Alive {\t\t\tavailableServers = append(availableServers, server)\t\t}\t\tserver.mutex.Unlock()\t}\tif len(availableServers) == 0 {\t\t// No available servers found, return nil\t\treturn nil\t}\t// return some random server from available servers\treturn availableServers[rand.Intn(len(availableServers))]}Now, let’s set up our main function to start the load balancer. In the main.go file, we define the backend servers as a slice of Server instances. Then, we create instances of the load balancers: RoundRobinLoadBalancer , LeastConnectionsLoadBalancer , and RandomLoadBalancer . We register each load balancer as an HTTP handler for a specific route. Finally, we start the HTTP server on port 8080.package mainimport (\t\"fmt\"\t\"log\"\t\"net/http\"\t\"os\")var logger *log.Loggerfunc init() {\t// Create the logger with desired settings\tlogger = log.New(os.Stdout, \"\", log.LstdFlags)}func main() {\t// Define the backend servers\tservers := []*Server{{\t\tURL: \"https://jsonplaceholder.typicode.com\", Weight: 1, Alive: true},\t\t{URL: \"https://httpbin.org\", Weight: 2, Alive: true},\t\t{URL: \"https://reqres.in\", Weight: 3, Alive: true},\t}\t// Create the load balancers\troundRobinLB := NewRoundRobinLB(servers)\tleastConnectionLB := NewLeastConnectionLB(servers)\trandomLB := NewRandomLB(servers)\t// Register the load balancers as HTTP handlers\thttp.Handle(\"/round-robin\", roundRobinLB)\thttp.Handle(\"/least-connections\", leastConnectionLB)\thttp.Handle(\"/random\", randomLB)\t// Start the server\tfmt.Println(\"Load balancers started.\")\terr := http.ListenAndServe(\":8080\", nil)\tif err != nil {\t\tfmt.Printf(\"Error starting server: %s\\n\", err.Error())\t}}How to run the server is mentioned in the README file. You can clone the full code from my GitHub repository.7. ConclusionLoad balancing plays a crucial role in modern distributed systems, ensuring optimal performance, scalability, and reliability. In this article, we explored load-balancing fundamentals and implemented three popular load-balancing algorithms in Go: Round Robin, Least Connections, and Random. We also discussed the differences between layer 4 and layer 7 load balancing.By understanding load-balancing algorithms and their implementations in Go, you can build efficient and scalable applications that can handle growing traffic demands. Whether you choose Round Robin, Least Connections, or Random load balancing, it’s important to consider your specific requirements and the characteristics of your backend servers.Post converted from Medium by ZMediumToMarkdown." }, { "title": "About", "url": "/posts/about/", "categories": "", "tags": "", "date": "2023-03-16 00:00:00 +0000", "snippet": "AboutThank you for using ZMediumToJekyll, this tool is powered by ZMediumToMarkdown, that is also create by ZhgChgLi.This tool can help you move your Medium posts to a Jekyll blog and keep them in ...", "content": "AboutThank you for using ZMediumToJekyll, this tool is powered by ZMediumToMarkdown, that is also create by ZhgChgLi.This tool can help you move your Medium posts to a Jekyll blog and keep them in sync in the future.It will automatically download your posts from Medium, convert them to Markdown, and upload them to your repository, check out my blog for online demo zhgchg.li.One-time setting, Lifetime enjoying❤️Powered by ZMediumToMarkdown, build on Jekyll with Chirpy theme.If you find this tool helpful, please consider to buy me a coffee.❤️Setup You can follow along with each step of this process by watching the following video tutorial Click the green button Use this template located above and select Create a new repository. Repo Owner could be an organization or username Enter the Repository Name, which usually uses your GitHub Username/Organization Name and ends with .github.io, for example, my organization name is zhgchgli than it’ll be zhgchgli.github.io. Select the public repository option, and then click on Create repository from template. Grant access to GitHub Actions by going to the Settings tab in your GitHub repository, selecting Actions -&gt; General, and finding the Workflow permissions section, then, select Read and write permissions, and click on Save to save the changes.*If you choose a different Repository Name, the GitHub page will be https://username.github.io/Repository Name instead of https://username.github.io/, and you will need to fill in the baseurl field in _config.yml with your Repository Name.*If you are using an organization and cannot enable Read and Write permissions in the repository settings, please refer to the organization settings page and enable it there.First-time run Please refer to the configuration information in the section below and make sure to specify your Medium username in the _zmediumtomarkdown.yml file. ⌛️ Please wait for the Automatic Build and pages-build-deployment gitHub actions to finish before making any further changes. Then, you can manually run the ZMediumToMarkdown GitHub action by going to the Actions tab in your GitHub repository, selecting the ZMediumToMarkdown action, clicking on the Run workflow button, and selecting the main branch. ⌛️ Please wait for the action to download and convert all Medium posts from the specified username, and commit the posts to your repository. ⌛️ Please wait for the Automatic Build and pages-build-deployment actions will also need to finish before making any further changes, and that they will start automatically once the ZMediumToMarkdown action has completed. Go to the Settings section of your GitHub repository and select Pages, In the Branch field, select gh-pages, and leave /(root) selected as the default. Click Save, you can also find the URL for your GitHub page at the top of the page. ⌛️ Please wait the Pages build and deployment action to finish. 🎉 After all actions are completed, you can visit your xxx.github.io page to verify that the results are correct. Congratulations! 🎉*To avoid expected Git conflicts or unexpected errors, please follow the steps carefully and in order, and be patient while waiting for each action to complete.*Note that the first time running may take longer.*If you open the URL and notice that something is wrong, such as the web style being missing, please ensure that your configuration in the _config.yml file is correct.*Please refer to the ‘Things to Know’ and ‘Troubleshooting’ sections below for more information.ConfigurationSite Setting_zmediumtomarkdown.ymlmedium_username: # enter your username on Medium.comPlease specify your Medium username for automatic download and syncing of your posts._config.yml &amp; jekyll settingFor more information, please refer to jekyll-theme-chirpy or jekyllrb.Github ActionZMediumToMarkdownYou can configure the time interval for syncing in ./.github/workflows/ZMediumToMarkdown.yml.The default time interval for syncing is once per day.You can also manually run the ZMediumToMarkdown action by going to the Actions tab in your GitHub repository, selecting the ZMediumToMarkdown action, clicking on the Run workflow button, and selecting the main branch.DisclaimerAll content downloaded using ZMediumToMarkdown, including but not limited to articles, images, and videos, are subject to copyright laws and belong to their respective owners. ZMediumToMarkdown does not claim ownership of any content downloaded using this tool.Downloading and using copyrighted content without the owner’s permission may be illegal and may result in legal action. ZMediumToMarkdown does not condone or support copyright infringement and will not be held responsible for any misuse of this tool.Users of ZMediumToMarkdown are solely responsible for ensuring that they have the necessary permissions and rights to download and use any content obtained using this tool. ZMediumToMarkdown is not responsible for any legal issues that may arise from the misuse of this tool.By using ZMediumToMarkdown, users acknowledge and agree to comply with all applicable copyright laws and regulations.TroubleshootingMy GitHub page keeps presenting a 404 error or doesn’t update with the latest posts. Please make sure you have followed the setup steps above in order. Wait for all GitHub actions to finish, including the Pages build and deployment and Automatic Build actions, you can check the progress on the Actions tab. Make sure you have the correct settings selected in Settings -&gt; Pages.Things to know The ZMediumToMarkdown GitHub Action for syncing Medium posts will automatically run every day by default, and you can also manually trigger it on the GitHub Actions page or adjust the sync frequency as needed. Every commit and post change will trigger the Automatic Build &amp; Pages build and deployment action. Please wait for this action to finish before checking the final result. You can create your own Markdown posts in the _posts directory by naming the file as YYYY-MM-DD-POSTNAME and recommend using lowercase file names. You can include images and other resources in the /assets directory. Things to know The ZMediumToMarkdown GitHub Action for syncing Medium posts will automatically run every day by default, and you can also manually trigger it on the GitHub Actions page or adjust the sync frequency as needed. Every commit and post change will trigger the Automatic Build &amp; Pages build and deployment action. Please wait for this action to finish before checking the final result. You can create your own Markdown posts in the _posts directory by naming the file as YYYY-MM-DD-POSTNAME and recommend using lowercase file names. You can include images and other resources in the /assets directory. Also, if you would like to remove the ZMediumToMarkdown watermark located at the bottom of the post, you may do so. I don’t mind. You can edit the Ruby file at tools/optimize_markdown.rb and uncomment lines 10-12. This will automatically remove the ZMediumToMarkdown watermark at the end of all posts during Jekyll build time. Since ZMediumToMarkdown is not an official tool and Medium does not provide a public API for it, I cannot guarantee that the parser target will not change in the future. However, I have tried to test it for as many cases as possible. If you encounter any rendering errors or Jekyll build errors, please feel free to create an issue and I will fix them as soon as possible." }, { "title": "Dear Myself", "url": "/posts/f57e38bd459f/", "categories": "", "tags": "self-awareness, self-love", "date": "2022-07-10 10:50:02 +0000", "snippet": "Dear MyselfDear Maya , I stumbled on this movie while randomly scrolling, with a few expectations.It is really an engrossing movie, based on the life of Maya Devi, a lonely and old woman who has lo...", "content": "Dear MyselfDear Maya , I stumbled on this movie while randomly scrolling, with a few expectations.It is really an engrossing movie, based on the life of Maya Devi, a lonely and old woman who has lost all hopes to reclaim a life that is always there to begin. Then two school girls decided to add color to her life by sending fake love letters. Little did they know how it is going to impact their lives.Just a few notes to myself after watching this movie :Dear Myself , Whatever destiny has in store for you, always say yes to life. Because you have no idea what is going to happen tomorrow.Dear Myself , Be courageous enough to try something out of your comfort zone Don’t be afraid, instead enjoy the process and lessons you learn along the way.Dear Myself , Always try to live in the present moment. Trying to change it will lead only to frustration and exhaustion. Acceptance is the key to a happy life. Acceptance of an unpleasant situation does not mean giving up on your goals. We are just trying to accept the situation which is beyond our control.Dear Myself, Don’t fall into the trap of sacrificing your self-esteem for affection and acceptance from others.That’s easier said than done. Practice makes a man perfect. Personally, I feel meditation helps a lot in achieving this. “I am quite naive, all wishes will come true. Wonder when someone will come with a smile.”Post converted from Medium by ZMediumToMarkdown." }, { "title": "Think, Stick and just go", "url": "/posts/ed48312c0e80/", "categories": "Women, in, Technology", "tags": "inspirational, insights, technology, woman-in-tech, women-in-technology", "date": "2021-06-17 17:14:13 +0000", "snippet": "Think, Stick and just goI just finished watching the Skater Girl movie and have penned down my thoughts.First of all, it’s really an inspiring movie. It focuses on the story of a poor village girl,...", "content": "Think, Stick and just goI just finished watching the Skater Girl movie and have penned down my thoughts.First of all, it’s really an inspiring movie. It focuses on the story of a poor village girl, who initially had no idea what to do in her life and soon introduced to the world of skateboarding by a foreigner. But she faces a rough road ahead but never gives up and follows her passion.It really reminded me of my journey. Being a small-town girl, I faced resistance at every point in my life. But I never gave up. Even now, a lot of people do not appreciate my work, being a part of a male-dominated industry.I too feel the scarcity of women in the tech world. To date in my career, I have been to startups where I was the only female developer. It might be due to the gender stereotype and lack of a talent pool. A very few women prefer or rather allowed to explore technology-related fields. We really need to change our mindset.“Women bring a lot to a tech company — different perspectives and skillsets, tech companies with more women are more successful and it’s a hot industry to be in, good-paying jobs with lots of diversity in focus (dev, cybersecurity, ops, etc. ) and opportunity. Don’t allow yourself to be intimidated by a room full of men.”Last but not least, I just wanted to say — Give wings to your dreamsFew lines from the movie :“I have touched the pinnacle,I have realized my worth,Now that my feet have opened up as wings , why shouldn’t I weave,The dreams my eyes have seen, My complicated life I have to resolve,The trust I have in me has to manifest,I will touch the sky, immersed in joy,I have taken flight. I know sun is gonna shine on me”Post converted from Medium by ZMediumToMarkdown." }, { "title": "Man’s search for meaning is an autobiography of Holocaust survivor Viktor Frankyl.", "url": "/posts/408a9935ac98/", "categories": "", "tags": "novel, novel-review, viktor-frankl", "date": "2021-05-13 12:40:00 +0000", "snippet": "Man’s search for meaningMan’s search for meaning is an autobiography of Holocaust survivor Viktor Frankyl . He wrote the book within nine consecutive days, with the firm intention of publishing it ...", "content": "Man’s search for meaningMan’s search for meaning is an autobiography of Holocaust survivor Viktor Frankyl . He wrote the book within nine consecutive days, with the firm intention of publishing it anonymously, but upon his friend’s insistent urge, he agreed to add his name.This book is divided into two sections : Experiences in a Concentration Camp : In this author gives vivid detail of his life in various concentration camps during World War II and how he narrowly survived. How guards in the camp took every belonging away from him but not his inner freedom to respond to the situation. “Everything can be taken from a man but one thing: the last of the human freedoms — to choose one’s attitude in any given set of circumstances, to choose one’s own way.”The author wants to concentrate less on the atrocities of Nazis, what he suffered and lost but more on the strength that kept him alive. “He who has a Why to live for can bear any How.”He observed the way a prisoner imagined the future affected his longevity. Prisoners, who had lost all the hopes for a future and had no meaning to life were the first ones to die. They died less from the lack of food or medicine than from lack of hope or meaning in life.2. Logotherapy in a Nutshell: The second part gets a bit clinical presents an existential analysis. He had started working on developing Logotherapy (Therapy through meaning) before he was imprisoned by the Nazis. Here he explains with examples how he applied this theory to himself as well as on his fellow prisoners to survive in camps. “Give opportunity to be proud of your sufferings and to consider it ennobling rather than degrading. Find meaning in your sufferings.”Professor Frankl indicates that we can discover this meaning in life in three different ways: (1) by creating a work or doing a deed (2) by experiencing something or encountering someone and (3) by the attitude we take toward unavoidable suffering.This book contains a wealth of ideas and it will remain next to me always.Read it, talk about itPS : My favorite part is where he talked about his wife knowing that she might not be alive at that moment. “I did not know whether my wife was alive, and I had no means of finding out (during all my prison life there was no outgoing or incoming mail), but at that moment it ceased to matter. There was no need for me to know; nothing could touch the strength of my love, my thoughts, and the image of my beloved. Had I known then that my wife was dead, I think that I would still have given myself, undisturbed by that knowledge, to the contemplation of her image, and that my mental conversation with her would have been just as vivid and just as satisfying. Set me like a seal upon thy heart, love is as strong as death.”Post converted from Medium by ZMediumToMarkdown." }, { "title": "Find the majority element using the Boyer–Moore algorithm", "url": "/posts/a464a5c9e01c/", "categories": "", "tags": "linear-data-structure, algorithms, boyer-moore", "date": "2020-03-09 08:34:35 +0000", "snippet": "Find the majority element using the Boyer–Moore algorithmIf you are here, chances are you are trying to solve the “ Find majority element in an array ” problem and came across the term Boyer-Moore ...", "content": "Find the majority element using the Boyer–Moore algorithmIf you are here, chances are you are trying to solve the “ Find majority element in an array ” problem and came across the term Boyer-Moore algorithm .Let’s fast forward to the problem description : The majority element is an element in an array that occurs more than (size/2) times in an array (where size​ is the number of elements stored in an array) .For Example, Majority element is 3 in the array {3,6,7,3,45,3,5,3,3}Now let’s have a look at the basic approaches first.Brute ForceUse nested loops and count the frequency of each element. If we reach an element with a frequency greater than n/2, that’s our majority element. Complexity AnalysisTime Complexity: O(n²)Space Complexity: O(1)Use SortingSort the array, all the similar elements will be next to each other. We can easily check the frequency of each element using the starting position and ending position of the respective element. Complexity AnalysisTime Complexity: Sorting + Linear Traversal (Here each element is visited only once) = O(nlogn) + O(n) = O(nlogn)Space Complexity: O(n) (In case of merge sort)Use HashmapStore the count of occurrences of an element and return the element with a count greater than (size/2) Complexity AnalysisTime Complexity: O(n)Space Complexity: O(n)Boyer-Moore AlgorithmWe can find the majority element in linear time and constant space using this algorithm. It requires exactly 2 passes over the input list. Simple to implement, little trickier to understand.In the first pass, we need two parameters, A candidate value(initially set to any value), and a count( store the occurrences of candidate value, set to zero initially)For each element in the array, compare it to the current candidate value. If they are the same, we increment count by 1. If they are different, we decrement count by 1. If count becomes zero, we change the candidate with the element at the current index.A second O(N) pass can verify that the candidate is the majority element.How does it workTry to think of it as a war, where a number of groups are fighting with each other. In our case(shown below), there are 4 different groups(A,B,C,D) . Any soldier can kill another group’s soldier by killing himself. In the end, whatever group is left with more than half soldiers, is the winner of the war.Try to relate it with the below diagram:In partially pairing, the soldier of group B is killed by group C. Group A is left with more than half the soldiers, hence, the winner.The second iteration is to verify the count of the element (found in the first iteration)If you want to check the mathematical proof for this approach, please check the linkPost converted from Medium by ZMediumToMarkdown." }, { "title": "Give wings to your dream", "url": "/posts/bb177b612780/", "categories": "", "tags": "hopes-and-dreams, gate, gate-exam", "date": "2020-03-02 17:47:57 +0000", "snippet": "Give wings to your dreamToday, I am going to share my thoughts and experience related to Gate Preparation, hoping that it will be beneficial for those who are preparing for the same and chasing the...", "content": "Give wings to your dreamToday, I am going to share my thoughts and experience related to Gate Preparation, hoping that it will be beneficial for those who are preparing for the same and chasing their dream.I completed my B.Tech from Punjabi University Patiala. At that time, I had a mindset that the persons, who are studying in good colleges like THAPAR, PEC, NIT’s, IIT’s are brilliant and extra-ordinary. It is very difficult to get into these colleges. This thinking always deters me from achieving something big. But one of my friends from THAPAR always inspired me and used to mooted this issue that if anyone else can do this, then why can’t you ?? This rekindled the HOPE again in me :) :) After getting placed in INFOSYS, I was skeptical that Was it the aim that I wanted to achieve in my life? And started thinking about higher education. So it is my serious advice for all those students, whose thinking is the same as I had, here nothing matters more than your “HARDWORK”. Just give your best and everything else will be yours :) :)Until the last year of B.Tech, I was not much conscious about the GATE. I started preparing for GATE on my own. But the issue was that my concepts were not clear and I studied 3–4 subjects only. I was not much confident at that time and appeared for GATE 2012. The result was out and I was too :P :P But my mind was prepared for it.So, By that time I strongly realized that I need guidance and coaching. So, I made a thought of joining MADE EASY DELHI. After joining class, I felt that my concepts were seriously very weak. So, I noticed that I need a lot of practice and hard work to improve and sharpen my concepts. During the initial months of my coaching, I was not that serious and I did not care about my preparation until I scored very less in the class test. After that, I got to know about my level of preparation and then I started really working hard. I made a schedule according to the subjects. I used to read standard books of related subjects and solved the back exercise. I collected questions of CSE from last year’s GATE papers and practiced those questions. This helped a lot and I joined MADE EASY and GATE FORUM test series. Videos from NPTEL site is also a good source for learning. Slowly-2, I gained confidence and Started dreaming for IIT. At the end of JAN, I revised all the subjects and my performance was quite good in Test Series.Finally, the day came, although this time I was confident, nervousness overcame this, an assorted feeling. I appeared for GATE 2013. This time paper was tough as compared to previous papers. I did my best but in the haste did some silly mistakes also :( :( After the paper, I was sad because I knew I could do better than this.Finally, the most awaited day came 15 MARCH. The result was out and this time I was in :) :) I was satisfied by scoring 339 AIR. Thanks to Almighty for his benign gaze on me. But I was expecting rank within 200. Still, I was satisfied with my toil now :) :)After that, I got a call from IISC for Computational Science and Engineering. But this course is much inclined towards Maths. It is not a cup of tea for those who are not interested in Mathematics. I did not appear for the written test. After that, I got call from IIT KANPUR for an interview. I reached Kanpur a day before the written test. The syllabus for the test was the same as that of the GATE. Aptitude, Probability, ALGO, and TOC was in high quantity. I attempted near about 35 questions out of 40. This time, I was satisfied with my performance. They shortlisted 110 students for the interview and I was among one of them.The interview was on the next day. There were 2 professors in the interview panel. One of them asked me about my favorite subjects. I replied TOC and ALGO. Then he asked about PUNJABI UNIVERSITY. I replied.They asked the following questions : Implement the Indian Number System with the help of automata. — I drew the FA on the board. He seemed to be not happy with it, he asked me “Do you know about the regular language?” I said “Yes”. After that, he asked me to write the regular grammer or regular expression. After thinking for a while I explained it on the board. This time he was satisfied with my answer to some extent. Then he asked some questions related to TM, Semi-Infinite TM and 2–3 more. I answered all. Implement queue with the help of stack. — This was a cakewalk for me :) I explained it and its complexities on the board. He told another professor to ask a question but he said no need.Finally, my Interview was over. After 5–6 days I got an mail that I was selected for M.Tech in CSE in IITK. I was on the ninth cloud on that day and finally, my DREAM come true :) :)With Regards,KAMNAPost converted from Medium by ZMediumToMarkdown." } ]
